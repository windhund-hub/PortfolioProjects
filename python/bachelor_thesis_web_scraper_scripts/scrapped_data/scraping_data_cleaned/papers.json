[{
  "_id": {
    "$oid": "60dae7e7b3da9102496b2356"
  },
  "type": "paper-conference",
  "abstract": "Machine-learning (ML) software engineering design patterns encapsulate reusable solutions to commonly occurring problems within the given contexts of ML systems and software design. These ML patterns should help develop and maintain ML systems and software from the design perspective. However, to the best of our knowledge, there is no study on the practitioners' insights on the use of ML patterns for design of their ML systems and software. Herein we report the preliminary results of a literature review and a questionnaire-based survey on ML system developers' state-of-practices with concrete ML patterns.",
  "container-title": "2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
  "DOI": "10.1109/ICSME46990.2020.00095",
  "event": "2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
  "note": "ISSN: 2576-3148",
  "page": "797-799",
  "source": "IEEE Xplore",
  "title": "Practitioners’ insights on machine-learning software engineering design patterns: a preliminary study",
  "title-short": "Practitioners’ insights on machine-learning software engineering design patterns",
  "author": [
    {
      "family": "Washizaki",
      "given": "H."
    },
    {
      "family": "Takeuchi",
      "given": "H."
    },
    {
      "family": "Khomh",
      "given": "F."
    },
    {
      "family": "Natori",
      "given": "N."
    },
    {
      "family": "Doi",
      "given": "T."
    },
    {
      "family": "Okuda",
      "given": "S."
    }
  ],
  "year": "2020",
  "categorie": [
    "Architecture Design"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b2357"
  },
  "type": "paper-conference",
  "abstract": "Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.",
  "container-title": "2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)",
  "DOI": "10.1145/3377811.3380362",
  "event": "2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)",
  "note": "ISSN: 1558-1225",
  "page": "1159-1170",
  "source": "IEEE Xplore",
  "title": "An Empirical Study on Program Failures of Deep Learning Jobs",
  "author": [
    {
      "family": "Zhang",
      "given": "R."
    },
    {
      "family": "Xiao",
      "given": "W."
    },
    {
      "family": "Zhang",
      "given": "H."
    },
    {
      "family": "Liu",
      "given": "Y."
    },
    {
      "family": "Lin",
      "given": "H."
    },
    {
      "family": "Yang",
      "given": "M."
    }
  ],
  "year": "2020",
  "categorie": [
    "AI Software Quality",
    "Testing",
    "Infrastructure",
    "Model Development"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b2358"
  },
  "type": "paper-conference",
  "abstract": "Machine learning and deep learning techniques are becoming increasingly popular and critical for companies as part of their systems. However, although the development and prototyping of ML/DL systems are common across companies, the transition from prototype to production-quality deployment models are challenging. One of the key challenges is how to determine the selection of an optimal architecture for AI deployment. Based on our previous research, and to offer support and guidance to practitioners, we developed a framework in which we present five architectural alternatives for AI deployment ranging from centralized to fully decentralized edge architectures. As part of our research, we validated the framework in software-intensive embedded system companies and identified key challenges they face when deploying ML/DL models. In this paper, and to further advance our research on this topic, we identify factors that help practitioners determine what architecture to select for the ML/D L model deployment. For this, we conducted a follow-up study involving interviews and workshops in seven case companies in the embedded systems domain. Based on our findings, we identify three key factors and develop a framework in which we outline how prioritization and trade-offs between these results in certain architecture. The contribution of the paper is threefold. First, we identify key factors critical for AI system deployment. Second, we present the architecture selection framework that explains how prioritization and trade-offs between key factors result in the selection of a certain architecture. Third, we discuss additional factors that mayor may not influence the selection of an optimal architecture.",
  "container-title": "2020 27th Asia-Pacific Software Engineering Conference (APSEC)",
  "DOI": "10.1109/APSEC51365.2020.00048",
  "event": "2020 27th Asia-Pacific Software Engineering Conference (APSEC)",
  "note": "ISSN: 2640-0715",
  "page": "395-404",
  "source": "IEEE Xplore",
  "title": "AI Deployment Architecture: Multi-Case Study for Key Factor Identification",
  "title-short": "AI Deployment Architecture",
  "author": [
    {
      "family": "John",
      "given": "M. M."
    },
    {
      "family": "Olsson",
      "given": "H. H."
    },
    {
      "family": "Bosch",
      "given": "J."
    }
  ],
  "year": "2020",
  "categorie": [
    "Model Deployment"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b2359"
  },
  "type": "paper-conference",
  "abstract": "In recent years, the application of artificial intelligence (AI) has become an integral part of a wide range of areas, including software engineering. By analyzing various data sources generated in software engineering, it can provide valuable insights into customer behavior, product performance, bugs and errors, and many more. In practice, however, AI for software analytics and business intelligence often gets stuck in a prototypical stage and the results are rarely used to make decisions based on data. To understand the underlying root causes of this phenomenon, we conduct both an explanatory case study and a survey on the challenges of realizing and utilizing artificial intelligence in the context of software-intensive businesses. As a result, we identify a vicious circle that prevents practitioners from moving from prototypical analytics to continuous and productively usable software analytics and business intelligence based on AI.",
  "container-title": "2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "DOI": "10.1109/SEAA51224.2020.00013",
  "event": "2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "page": "5-12",
  "source": "IEEE Xplore",
  "title": "Breaking the Vicious Circle: Why AI for software analytics and business intelligence does not take off in practice",
  "title-short": "Breaking the Vicious Circle",
  "author": [
    {
      "family": "Figalist",
      "given": "I."
    },
    {
      "family": "Elsner",
      "given": "C."
    },
    {
      "family": "Bosch",
      "given": "J."
    },
    {
      "family": "Olsson",
      "given": "H. H."
    }
  ],
  "year": "2020",
  "categorie": [
    "Architecture Design",
    "Project Management"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235a"
  },
  "type": "paper-conference",
  "abstract": "Data augmentation techniques that increase the amount of training data by adding realistic transformations are used in machine learning to improve the level of accuracy. Recent studies have demonstrated that data augmentation techniques improve the robustness of image classification models with open datasets; however, it has yet to be investigated whether these techniques are effective for industrial datasets. In this study, we investigate the feasibility of data augmentation techniques for industrial use. We evaluate data augmentation techniques in image classification and object detection tasks using an industrial in-house graphical user interface dataset. As the results indicate, the genetic algorithm-based data augmentation technique outperforms two random-based methods in terms of the robustness of the image classification model. In addition, through this evaluation and interviews with the developers, we learned following two lessons: data augmentation techniques should (1) maintain the training speed to avoid slowing the development and (2) include extensibility for a variety of tasks.",
  "container-title": "2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
  "event": "2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
  "note": "ISSN: 2643-1572",
  "page": "1184-1188",
  "source": "IEEE Xplore",
  "title": "Towards Building Robust DNN Applications: An Industrial Case Study of Evolutionary Data Augmentation",
  "title-short": "Towards Building Robust DNN Applications",
  "author": [
    {
      "family": "Yokoyama",
      "given": "H."
    },
    {
      "family": "Onoue",
      "given": "S."
    },
    {
      "family": "Kikuchi",
      "given": "S."
    }
  ],
  "year": "2020",
  "categorie": [
    "AI Software Quality"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235b"
  },
  "type": "paper-conference",
  "abstract": "The rise of big data has led to an increase in data science projects conducted by organizations. Such projects aim to create valuable insights by improving decision making or enhancing an organization's service offering through data-driven services. However, the majority of data science projects still fail to deliver the expected value. To increase the success rate of projects, the use of process models or methodologies is recommended in the literature. Nevertheless, organizations are hardly using them because they are considered too rigid and they do not support the typical iterative and open nature of data science projects. To overcome this problem, this research suggests applying Agile methodologies to data science projects. Agile methodologies were originally developed in the software engineering domain and are characterised by their iterative approach towards software development. In this study, we selected the Scrum approach and integrated it into the CRISP-DM methodology for data science projects using a Design Science Research approach. This new methodology was then evaluated in three different case organizations using expert interviews. Analysis of the expert interviews resulted in a further refinement of the Agile data science methodology proposed by this research.",
  "container-title": "2020 IEEE 22nd Conference on Business Informatics (CBI)",
  "DOI": "10.1109/CBI49978.2020.00011",
  "event": "2020 IEEE 22nd Conference on Business Informatics (CBI)",
  "note": "ISSN: 2378-1971",
  "page": "30-38",
  "source": "IEEE Xplore",
  "title": "Applying Scrum in Data Science Projects",
  "volume": "1",
  "author": [
    {
      "family": "Baijens",
      "given": "J."
    },
    {
      "family": "Helms",
      "given": "R."
    },
    {
      "family": "Iren",
      "given": "D."
    }
  ],
  "year": "2020",
  "categorie": [
    "Project Management",
    "Requirement Engineering",
    "AI Engineering"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235c"
  },
  "type": "article-journal",
  "abstract": "High-level guidelines and tools for managing artificial intelligence (AI) ethics have been introduced to help industry organizations make more ethical AI systems. The results of a survey of 211 software companies provide insights into the current state of industrial practice.",
  "container-title": "IEEE Software",
  "DOI": "10.1109/MS.2020.2985621",
  "ISSN": "1937-4194",
  "issue": "4",
  "note": "event: IEEE Software",
  "page": "50-57",
  "source": "IEEE Xplore",
  "title": "The Current State of Industrial Practice in Artificial Intelligence Ethics",
  "volume": "37",
  "author": [
    {
      "family": "Vakkuri",
      "given": "V."
    },
    {
      "family": "Kemell",
      "given": "K."
    },
    {
      "family": "Kultanen",
      "given": "J."
    },
    {
      "family": "Abrahamsson",
      "given": "P."
    }
  ],
  "year": "2020",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235d"
  },
  "type": "paper-conference",
  "abstract": "Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and enumerate the gaps in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders. The goal of this paper is to layout the research agenda to amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era.",
  "container-title": "2020 IEEE Security and Privacy Workshops (SPW)",
  "DOI": "10.1109/SPW50608.2020.00028",
  "event": "2020 IEEE Security and Privacy Workshops (SPW)",
  "page": "69-75",
  "source": "IEEE Xplore",
  "title": "Adversarial Machine Learning-Industry Perspectives",
  "author": [
    {
      "family": "Kumar",
      "given": "R. S. Siva"
    },
    {
      "family": "Nyström",
      "given": "M."
    },
    {
      "family": "Lambert",
      "given": "J."
    },
    {
      "family": "Marshall",
      "given": "A."
    },
    {
      "family": "Goertzel",
      "given": "M."
    },
    {
      "family": "Comissoneru",
      "given": "A."
    },
    {
      "family": "Swann",
      "given": "M."
    },
    {
      "family": "Xia",
      "given": "S."
    }
  ],
  "year": "2020",
  "categorie": [
    "AI Software Quality"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235e"
  },
  "type": "paper-conference",
  "abstract": "As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.",
  "container-title": "2020 IEEE 28th International Requirements Engineering Conference (RE)",
  "DOI": "10.1109/RE48521.2020.00036",
  "event": "2020 IEEE 28th International Requirements Engineering Conference (RE)",
  "note": "ISSN: 2332-6441",
  "page": "260-270",
  "source": "IEEE Xplore",
  "title": "Requirements-Driven Method to Determine Quality Characteristics and Measurements for Machine Learning Software and Its Evaluation",
  "author": [
    {
      "family": "Nakamichi",
      "given": "K."
    },
    {
      "family": "Ohashi",
      "given": "K."
    },
    {
      "family": "Namba",
      "given": "I."
    },
    {
      "family": "Yamamoto",
      "given": "R."
    },
    {
      "family": "Aoyama",
      "given": "M."
    },
    {
      "family": "Joeckel",
      "given": "L."
    },
    {
      "family": "Siebert",
      "given": "J."
    },
    {
      "family": "Heidrich",
      "given": "J."
    }
  ],
  "year": "2020",
  "categorie": [
    "Requirement Engineering",
    "AI Software Quality"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae7e7b3da9102496b235f"
  },
  "type": "paper-conference",
  "abstract": "System logs perform a critical function in software-intensive systems as logs record the state of the system and significant events in the system at important points in time. Unfortunately, log entries are typically created in an ad-hoc, unstructured and uncoordinated fashion, limiting their usefulness for analytics and machine learning. In a DevOps environment, especially, unmanaged evolution in log data structure causes frequent disruption of operations in automated data pipelines, dashboards and analytics. In this paper, we present the main challenges of contemporary approaches to generating, storing and managing the evolution of system logs data for large, complex, software-intensive systems based on an in-depth case study at a world-leading telecommunications company. Second, we present an approach for generating and managing the evolution of log data in a DevOps environment that does not suffer from the aforementioned challenges and is optimized for use in machine learning. Third, we provide validation of the approach based on expert interviews that confirm that the approach addresses the identified challenges and problems.",
  "container-title": "2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "DOI": "10.1109/SEAA51224.2020.00016",
  "event": "2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "page": "29-33",
  "source": "IEEE Xplore",
  "title": "Software Logs for Machine Learning in a DevOps Environment",
  "author": [
    {
      "family": "Bosch",
      "given": "Nathan"
    },
    {
      "family": "Bosch",
      "given": "Jan"
    }
  ],
  "year": "2020",
  "categorie": [
    "Infrastructure"
  ],
  "DB": "IEEE"
},{
  "_id": {
    "$oid": "60dae981b3da9102496b2360"
  },
  "type": "paper-conference",
  "abstract": "Finding a balance between testing goals and testing resources can be considered as a most challenging issue, therefore test optimization plays a vital role in the area of software testing. Several parameters such as the objectives of the tests, test cases similarities and dependencies between test cases need to be considered, before attempting any optimization approach. However, analyzing corresponding testing artifacts (e.g. requirement specification, test cases) for capturing the mentioned parameters is a complicated task especially in a manual testing procedure, where the test cases are documented as a natural text written by a human. Thus, utilizing artificial intelligence techniques in the process of analyzing complex and sometimes ambiguous test data, is considered to be working in different industries. Test scheduling is one of the most popular and practical ways to optimize the testing process. Having a group of test cases which are required the same system setup, installation or testing the same functionality can lead to a more efficient testing process. In this paper, we propose, apply and evaluate a natural language processing-based approach that derives test cases' similarities directly from their test specification. The proposed approach utilizes the Levenshtein distance and converts each test case into a string. Test cases are then grouped into several clusters based on their similarities. Finally, a set of cluster-based parallel test scheduling strategies are proposed for execution. The feasibility of the proposed approach is studied by an empirical evaluation that has been performed on a Telecom use-case at Ericsson in Sweden and indicates promising results.",
  "container-title": "2020 IEEE International Conference On Artificial Intelligence Testing (AITest)",
  "DOI": "10.1109/AITEST49225.2020.00022",
  "event": "2020 IEEE International Conference On Artificial Intelligence Testing (AITest)",
  "page": "99-106",
  "source": "IEEE Xplore",
  "title": "Cluster-Based Parallel Testing Using Semantic Analysis",
  "author": [
    {
      "family": "Landing",
      "given": "C."
    },
    {
      "family": "Tahvili",
      "given": "S."
    },
    {
      "family": "Haggren",
      "given": "H."
    },
    {
      "family": "Langkvis",
      "given": "M."
    },
    {
      "family": "Muhammad",
      "given": "A."
    },
    {
      "family": "Loufi",
      "given": "A."
    }
  ],
  "year": "2020",
  "DB": "IEEE",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60dae981b3da9102496b2361"
  },
  "type": "paper-conference",
  "abstract": "Project management planning assessment is of great significance in project performance activities. The creation of project management cannot be effectively handled without a practical and rational strategy. This paper offers a large-scale review analysis of articles based on machine learning and risk evaluation management for software projects. The reviews are presented and classified into two groups. The first group covers project management analysis and survey articles. The second group contains works on the steps and experimental criteria that are widely used in the management of machine learning projects. The paper provides a deeper insight and an important framework for future work in the project risk assessment, highlights the estimation of project risk using machine-learning is more efficient in reducing the project's fault and provides a further way to reduce the probability chances effectively and to increase the software development performance ratio.",
  "container-title": "2020 8th International Conference on Information Technology and Multimedia (ICIMU)",
  "DOI": "10.1109/ICIMU49871.2020.9243543",
  "event": "2020 8th International Conference on Information Technology and Multimedia (ICIMU)",
  "page": "363-370",
  "source": "IEEE Xplore",
  "title": "Software Project Management Using Machine Learning Technique - A Review",
  "author": [
    {
      "family": "H",
      "given": "M. Z. M."
    },
    {
      "family": "Mahdi",
      "given": "M. N."
    },
    {
      "family": "Azmi",
      "given": "M. S. Mohd"
    },
    {
      "family": "Cheng",
      "given": "L. K."
    },
    {
      "family": "Yusof",
      "given": "A."
    },
    {
      "family": "Ahmad",
      "given": "A. R."
    }
  ],
  "year": "2020",
  "DB": "IEEE",
  "categorie": [
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60dae981b3da9102496b2362"
  },
  "type": "paper-conference",
  "abstract": "This paper proposes an empirical study of exploring relevant and operable metrics to assess software product quality based on quality characteristics of ISO/IEC 25010. Both data-driven and Goal-Question-Metric approach are applied to excavate as many full-life-cycle metrics as possible. Goal-Question-Metric approach is applied to identify software metrics affecting product quality in a specific domain. Machine learning algorithm that support incremental training is applied to learn the relationship between code metrics and quality characteristic marks from historical data. Thus full-life-cycle software metrics are identified and we build a software quality assessment model based on historical code metric data.Two case studies are conducted based on actual projects from the past 2 years to verify the feasibility of methodology, including an empirical investigation of mostly concerned quality factors in an electronic equipment software institute, a collection of code metrics and quality characteristic marks of 82 aviation embedded software projects, as well as a comparation of the performance of logistic regression, k-Nearest Neighbor and BP neural network algorithm for quality assessment. Those cases are performed with actual data in actual institutes and projects. Additionally, a support tool for project data management and quality assessment is developed.Through feedback from cooperative engineers, we still see room for improvements to fill the gap between methodology and actual software engineering process. Nevertheless, the empirical validation shows the feasibility of the metrics for quality assessment.",
  "container-title": "2020 7th International Conference on Dependable Systems and Their Applications (DSA)",
  "DOI": "10.1109/DSA51864.2020.00023",
  "event": "2020 7th International Conference on Dependable Systems and Their Applications (DSA)",
  "page": "114-124",
  "source": "IEEE Xplore",
  "title": "An Empirical study of Exploring Relevant Metrics to Assess Software Product Quality",
  "author": [
    {
      "family": "Song",
      "given": "Z."
    },
    {
      "family": "Wang",
      "given": "Y."
    },
    {
      "family": "Wang",
      "given": "W."
    },
    {
      "family": "Zhang",
      "given": "J."
    }
  ],
  "year": "2020",
  "DB": "IEEE",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60dae981b3da9102496b2363"
  },
  "type": "article-journal",
  "abstract": "The growing diffusion of software in society and its influence on people demands from its creators that their work carefully considers human values such as transparency, social responsibility, and equality. But how do software practitioners address human values in software engineering practice’ We interviewed 31 software practitioners from two organizations, each having a strong values framework, with the aim to understand: (a) practitioners’ perceptions of human values and their role in software engineering; (b)practices that practitioners use to address human values in software; and (c) challenges they face during this process. We report our findings from two contrasting case studies on how practitioners “engineer” values in their unique organizational settings. We found evidence that organizational culture significantly contributes to how values are addressed in software. We summarize recommendations from the practitioners to support proactive engineering of values-conscious software.",
  "container-title": "IEEE Transactions on Software Engineering",
  "DOI": "10.1109/TSE.2020.3038802",
  "ISSN": "1939-3520",
  "note": "event: IEEE Transactions on Software Engineering",
  "page": "1-1",
  "source": "IEEE Xplore",
  "title": "Human Values in Software Engineering: Contrasting Case Studies of Practice",
  "title-short": "Human Values in Software Engineering",
  "author": [
    {
      "family": "Hussain",
      "given": "W."
    },
    {
      "family": "Perera",
      "given": "H."
    },
    {
      "family": "Whittle",
      "given": "J."
    },
    {
      "family": "Nurwidyantoro",
      "given": "A."
    },
    {
      "family": "Hoda",
      "given": "R."
    },
    {
      "family": "Shams",
      "given": "R. A."
    },
    {
      "family": "Oliver",
      "given": "G."
    }
  ],
  "year": "2020",
  "DB": "IEEE",
  "categorie": [
    "AI Software Quality",
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2364"
  },
  "type": "article-journal",
  "abstract": "Understanding what Machine Learning models are doing is not always trivial. This is especially true for complex models such as Deep Neural Networks (DNN), which are the best-suited algorithms for modeling very complex and nonlinear relationships. But this need to understand has become a must since privacy regulations are hardening the industrial use of these models. There are different techniques to address the interpretability issues that Machine Learning models arises. This paper is focused on opening the so-called Deep Neural architectures black-box. This research extends the technique called Layer-wise Relevant Propagation (LRP) enhancing its properties to compute the most critical paths in different deep neural architectures using multicriteria analysis. We call this technique Ranked-LRP and it was tested on four different datasets and tasks, including classification and regression. The results show the worth of our proposal.",
  "container-title": "Neurocomputing",
  "DOI": "10.1016/j.neucom.2019.11.059",
  "ISSN": "0925-2312",
  "journalAbbreviation": "Neurocomputing",
  "language": "en",
  "page": "252-260",
  "source": "ScienceDirect",
  "title": "LRP-Based path relevances for global explanation of deep architectures",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0925231219316613",
  "volume": "381",
  "author": [
    {
      "family": "Guerrero-Gómez-Olmedo",
      "given": "Ricardo"
    },
    {
      "family": "Salmeron",
      "given": "Jose L."
    },
    {
      "family": "Kuchkovsky",
      "given": "Carlos"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2365"
  },
  "type": "article-journal",
  "abstract": "Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research.",
  "container-title": "Patterns",
  "DOI": "10.1016/j.patter.2020.100049",
  "ISSN": "2666-3899",
  "issue": "4",
  "journalAbbreviation": "Patterns",
  "language": "en",
  "page": "100049",
  "source": "ScienceDirect",
  "title": "Rapid Trust Calibration through Interpretable and Uncertainty-Aware AI",
  "URL": "https://www.sciencedirect.com/science/article/pii/S266638992030060X",
  "volume": "1",
  "author": [
    {
      "family": "Tomsett",
      "given": "Richard"
    },
    {
      "family": "Preece",
      "given": "Alun"
    },
    {
      "family": "Braines",
      "given": "Dave"
    },
    {
      "family": "Cerutti",
      "given": "Federico"
    },
    {
      "family": "Chakraborty",
      "given": "Supriyo"
    },
    {
      "family": "Srivastava",
      "given": "Mani"
    },
    {
      "family": "Pearson",
      "given": "Gavin"
    },
    {
      "family": "Kaplan",
      "given": "Lance"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2366"
  },
  "type": "article-journal",
  "abstract": "Image analysis is key to extracting quantitative information from scientific microscopy images, but the methods involved are now often so refined that they can no longer be unambiguously described by written protocols. We introduce BIAFLOWS, an open-source web tool enabling to reproducibly deploy and benchmark bioimage analysis workflows coming from any software ecosystem. A curated instance of BIAFLOWS populated with 34 image analysis workflows and 15 microscopy image datasets recapitulating common bioimage analysis problems is available online. The workflows can be launched and assessed remotely by comparing their performance visually and according to standard benchmark metrics. We illustrated these features by comparing seven nuclei segmentation workflows, including deep-learning methods. BIAFLOWS enables to benchmark and share bioimage analysis workflows, hence safeguarding research results and promoting high-quality standards in image analysis. The platform is thoroughly documented and ready to gather annotated microscopy datasets and workflows contributed by the bioimaging community.",
  "container-title": "Patterns",
  "DOI": "10.1016/j.patter.2020.100040",
  "ISSN": "2666-3899",
  "issue": "3",
  "journalAbbreviation": "Patterns",
  "language": "en",
  "page": "100040",
  "source": "ScienceDirect",
  "title": "BIAFLOWS: A Collaborative Framework to Reproducibly Deploy and Benchmark Bioimage Analysis Workflows",
  "title-short": "BIAFLOWS",
  "URL": "https://www.sciencedirect.com/science/article/pii/S2666389920300453",
  "volume": "1",
  "author": [
    {
      "family": "Rubens",
      "given": "Ulysse"
    },
    {
      "family": "Mormont",
      "given": "Romain"
    },
    {
      "family": "Paavolainen",
      "given": "Lassi"
    },
    {
      "family": "Bäcker",
      "given": "Volker"
    },
    {
      "family": "Pavie",
      "given": "Benjamin"
    },
    {
      "family": "Scholz",
      "given": "Leandro A."
    },
    {
      "family": "Michiels",
      "given": "Gino"
    },
    {
      "family": "Maška",
      "given": "Martin"
    },
    {
      "family": "Ünay",
      "given": "Devrim"
    },
    {
      "family": "Ball",
      "given": "Graeme"
    },
    {
      "family": "Hoyoux",
      "given": "Renaud"
    },
    {
      "family": "Vandaele",
      "given": "Rémy"
    },
    {
      "family": "Golani",
      "given": "Ofra"
    },
    {
      "family": "Stanciu",
      "given": "Stefan G."
    },
    {
      "family": "Sladoje",
      "given": "Natasa"
    },
    {
      "family": "Paul-Gilloteaux",
      "given": "Perrine"
    },
    {
      "family": "Marée",
      "given": "Raphaël"
    },
    {
      "family": "Tosi",
      "given": "Sébastien"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "AI Software Quality",
    "Model Deployment",
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2367"
  },
  "type": "article-journal",
  "abstract": "The technological evolution of the smart grids is going to take its shape in the form of a new paradigm called the Internet of Energy (IoE); which is considered to be the convergence of internet, communication, and energy. Like other evolved technologies, the IoE inherits security vulnerabilities from its constituents that need to be addressed. Intrusion Detection Systems (IDS) have been used to counteract malicious attacks. Among the types of IDS, anomaly-based IDS that employ mostly machine learning algorithms are considered to be the promising one, owing to their capability of detecting zero-day attacks. However, using complex algorithms to detect attacks, the existing anomaly-based IDS designed for IoE require considerable amount of time. It is tempting to reduce the training and testing time in order to make the IDS feasible for the IoE architecture. In this paper, we propose a hybrid anomaly-based IDS that can be installed at any networked site of the IoE architecture, such as Advanced Metering Infrastructure (AMI), to counteract security attacks. Our proposed system reduces the overall classification time of detection compared to the existing hybrid methods. The proposed solution uses a combination of K-means and Support Vector Machine, where the K-means centroids are used in a unique training method that reduces the training and testing times of the Support Vector Machine without compromising classification performance. We choose the best value of “k” and fine-tuned the SVM for best anomaly detection. Our approach achieves the highest accuracy of 99.9% in comparison with the existing approaches.",
  "container-title": "Journal of Parallel and Distributed Computing",
  "DOI": "10.1016/j.jpdc.2020.06.012",
  "ISSN": "0743-7315",
  "journalAbbreviation": "Journal of Parallel and Distributed Computing",
  "language": "en",
  "page": "124-139",
  "source": "ScienceDirect",
  "title": "A hybrid anomaly-based intrusion detection system to improve time complexity in the Internet of Energy environment",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0743731520303191",
  "volume": "145",
  "author": [
    {
      "family": "Rose",
      "given": "Thomas"
    },
    {
      "family": "Kifayat",
      "given": "Kashif"
    },
    {
      "family": "Abbas",
      "given": "Sohail"
    },
    {
      "family": "Asim",
      "given": "Muhammad"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2368"
  },
  "type": "article-journal",
  "abstract": "Facial recognition technologies are implemented in many areas, including but not limited to, citizen surveillance, crime control, activity monitoring, and facial expression evaluation. However, processing biometric information is a resource-intensive task that often involves third-party servers, which can be accessed by adversaries with malicious intent. Biometric information delivered to untrusted third-party servers in an uncontrolled manner can be considered a significant privacy leak (i.e. uncontrolled information release) as biometrics can be correlated with sensitive data such as healthcare or financial records. In this paper, we propose a privacy-preserving technique for “controlled information release”, where we disguise an original face image and prevent leakage of the biometric features while identifying a person. We introduce a new privacy-preserving face recognition protocol named PEEP (Privacy using EigEnface Perturbation) that utilizes local differential privacy. PEEP applies perturbation to Eigenfaces utilizing differential privacy and stores only the perturbed data in the third-party servers to run a standard Eigenface recognition algorithm. As a result, the trained model will not be vulnerable to privacy attacks such as membership inference and model memorization attacks. Our experiments show that PEEP exhibits a classification accuracy of around 70% - 90% under standard privacy settings.",
  "container-title": "Computers & Security",
  "DOI": "10.1016/j.cose.2020.101951",
  "ISSN": "0167-4048",
  "journalAbbreviation": "Computers & Security",
  "language": "en",
  "page": "101951",
  "source": "ScienceDirect",
  "title": "Privacy Preserving Face Recognition Utilizing Differential Privacy",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0167404820302273",
  "volume": "97",
  "author": [
    {
      "family": "Chamikara",
      "given": "M. A. P."
    },
    {
      "family": "Bertok",
      "given": "P."
    },
    {
      "family": "Khalil",
      "given": "I."
    },
    {
      "family": "Liu",
      "given": "D."
    },
    {
      "family": "Camtepe",
      "given": "S."
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b2369"
  },
  "type": "article-journal",
  "abstract": "Recent works have demonstrated convolutional neural networks are vulnerable to adversarial examples, i.e., inputs to machine learning models that an attacker has intentionally designed to cause the models to make a mistake. To improve the adversarial robustness of neural networks, adversarial training has been proposed to train networks by injecting adversarial examples into the training data. However, adversarial training could overfit to a specific type of adversarial attack and also lead to standard accuracy drop on clean images. To this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for adversarial defense without directly applying adversarial training. Specifically, we propose to learn domain-invariant features for adversarial examples and clean images via a domain discriminator. Furthermore, we introduce a class-aware component into the discriminator to increase the discriminative power of the network for adversarial examples. We evaluate our newly proposed approach using multiple benchmark datasets. The results demonstrate that our method can significantly improve the state-of-the-art of adversarial robustness for various attacks and maintain high performances on clean images.",
  "container-title": "Image and Vision Computing",
  "DOI": "10.1016/j.imavis.2020.103926",
  "ISSN": "0262-8856",
  "journalAbbreviation": "Image and Vision Computing",
  "language": "en",
  "page": "103926",
  "source": "ScienceDirect",
  "title": "Class-aware domain adaptation for improving adversarial robustness",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0262885620300585",
  "volume": "99",
  "author": [
    {
      "family": "Hou",
      "given": "Xianxu"
    },
    {
      "family": "Liu",
      "given": "Jingxin"
    },
    {
      "family": "Xu",
      "given": "Bolei"
    },
    {
      "family": "Wang",
      "given": "Xiaolong"
    },
    {
      "family": "Liu",
      "given": "Bozhi"
    },
    {
      "family": "Qiu",
      "given": "Guoping"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b236a"
  },
  "type": "article-journal",
  "abstract": "Identifying the relevant set of features in a dataset is an important part of data analytics. Discarding significant variables or keeping irrelevant variables has significant effects on the performance of the learning algorithm during knowledge discovery. In this paper, a feature selection method called Least Loss (L2) is proposed that significantly reduces the dimensionality of data by disposing weakly correlated variables in a robust manner without diminishing the predictive performance of classifiers. The proposed method is based on quantifying the similarity between the observed and expected probabilities and generating scores for each independent variable, which makes it simple and intuitive. The evaluation of the proposed method was done by comparing its performance against Information Gain (IG) and Chi Square (CHI) feature selection methods on 27 different datasets modeled using a probabilistic classifier. The results reveal that L2 is highly competitive with respect to error rate, precision, and recall measures while substantially reducing the number of selected variables in the datasets. Our study would be of high interest to data analysts, scholars and domain experts who deal with applications that include large numbers of features using statistical analysis methods.",
  "container-title": "Information Sciences",
  "DOI": "10.1016/j.ins.2020.05.017",
  "ISSN": "0020-0255",
  "journalAbbreviation": "Information Sciences",
  "language": "en",
  "page": "1-15",
  "source": "ScienceDirect",
  "title": "Least Loss: A simplified filter method for feature selection",
  "title-short": "Least Loss",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0020025520304242",
  "volume": "534",
  "author": [
    {
      "family": "Thabtah",
      "given": "Fadi"
    },
    {
      "family": "Kamalov",
      "given": "Firuz"
    },
    {
      "family": "Hammoud",
      "given": "Suhel"
    },
    {
      "family": "Shahamiri",
      "given": "Seyed Reza"
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60daed76b3da9102496b236b"
  },
  "type": "article-journal",
  "abstract": "Hyper-parameter optimization is a process to find suitable hyper-parameters for predictive models. It typically incurs highly demanding computational costs due to the need of the time-consuming model training process to determine the effectiveness of each set of candidate hyper-parameter values. A priori, there is no guarantee that hyper-parameter optimization leads to improved performance. In this work, we propose a framework to address the problem of whether one should apply hyper-parameter optimization or use the default hyper-parameter settings for traditional classification algorithms. We implemented a prototype of the framework, which we use a basis for a three-fold evaluation with 486 datasets and 4 algorithms. The results indicate that our framework is effective at supporting modeling tasks in avoiding adverse effects of using ineffective optimizations. The results also demonstrate that incrementally adding training datasets improves the predictive performance of framework instantiations and hence enables “life-long learning.”",
  "container-title": "Pattern Recognition",
  "DOI": "10.1016/j.patcog.2020.107245",
  "ISSN": "0031-3203",
  "journalAbbreviation": "Pattern Recognition",
  "language": "en",
  "page": "107245",
  "source": "ScienceDirect",
  "title": "Hyper-parameter optimization in classification: To-do or not-to-do",
  "title-short": "Hyper-parameter optimization in classification",
  "URL": "https://www.sciencedirect.com/science/article/pii/S0031320320300510",
  "volume": "103",
  "author": [
    {
      "family": "Tran",
      "given": "Ngoc"
    },
    {
      "family": "Schneider",
      "given": "Jean-Guy"
    },
    {
      "family": "Weber",
      "given": "Ingo"
    },
    {
      "family": "Qin",
      "given": "A. K."
    }
  ],
  "year": "2020",
  "DB": "Science Direct",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b236c"
  },
  "type": "paper-conference",
  "abstract": "Major simultaneous disruptions are currently under way in both hardware and software. In hardware, ``extreme heterogeneity'' has become critical to sustaining cost and performance improvements after Moore's Law, but poses productivity and portability challenges for developers. In software, the rise of large-scale data science is driven by developers who come from diverse backgrounds and, moreover, who demand the rapid prototyping and interactive-notebook capabilities of high-productivity languages like Python. We introduce the Intrepydd programming system, which enables data scientists to write application kernels with high performance, productivity, and portability on current and future hardware. Intrepydd is based on Python, though the approach can be applied to other base languages as well. To deliver high performance, the Intrepydd toolchain uses ahead-of-time (AOT) compilation and high-level compiler optimizations of Intrepydd kernels. Intrepydd achieves portability by its ability to compile kernels for execution on different hardware platforms, and for invocation from Python or C++ main programs. An empirical evaluation shows significant performance improvements relative to Python, and the suitability of Intrepydd for mapping on to post-Moore accelerators and architectures with relative ease. We believe that Intrepydd represents a new direction of ``Discipline-Aware Languages'' (DiALs), which brings us closer to the holy grail of obtaining productivity and portability with higher performance than current Python-like languages, and with more generality than current domain-specific languages and libraries.",
  "collection-title": "Onward! 2020",
  "container-title": "Proceedings of the 2020 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software",
  "DOI": "10.1145/3426428.3426915",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-8178-9",
  "page": "65–83",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Intrepydd: performance, productivity, and portability for data science application kernels",
  "title-short": "Intrepydd",
  "URL": "https://doi.org/10.1145/3426428.3426915",
  "author": [
    {
      "family": "Zhou",
      "given": "Tong"
    },
    {
      "family": "Shirako",
      "given": "Jun"
    },
    {
      "family": "Jain",
      "given": "Anirudh"
    },
    {
      "family": "Srikanth",
      "given": "Sriseshan"
    },
    {
      "family": "Conte",
      "given": "Thomas M."
    },
    {
      "family": "Vuduc",
      "given": "Richard"
    },
    {
      "family": "Sarkar",
      "given": "Vivek"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b236d"
  },
  "type": "paper-conference",
  "abstract": "This paper presents a preliminary study for the hands-on creation of an intelligent decision support tool (IDST) for occupational health (OH) physicians. We addressed this challenge through an iterative design process consisting of three phases with different levels of stakeholder involvement, spanning from understanding the context to developing the concept and consolidating the design. We identified a set of design considerations that focused on enriching data collection, improving the accessibility of information, and blending the decision support into the workflow. To demonstrate these insights, we developed the concept of an AI-based OH consultation, called ConsultAI. ConsultAI is a conversational assistant that can provide real-time decision support to OH physicians during clinical interviews. Based on this case study, we discussed stakeholder engagement in the design of IDSTs for OH physicians.",
  "collection-title": "CHI EA '20",
  "container-title": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems",
  "DOI": "10.1145/3334480.3383061",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6819-3",
  "page": "1–8",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Engaging Stakeholders to Design an Intelligent Decision Support Tool in the Occupational Health Context",
  "URL": "https://doi.org/10.1145/3334480.3383061",
  "author": [
    {
      "family": "Ren",
      "given": "Xipei"
    },
    {
      "family": "Spina",
      "given": "Gabriele"
    },
    {
      "family": "Faber",
      "given": "Babs"
    },
    {
      "family": "Geraedts",
      "given": "Anna"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Requirement Engineering",
    "Architecture Design"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b236e"
  },
  "type": "paper-conference",
  "abstract": "Artificial intelligence (AI) is transforming care delivery and expanding precision medicine. This paper presents experiences of a 110-person, spread across three countries, involves multiple business units and external suppliers that successfully achieved multiple milestones. The product is an organization visionary software system, a mission-critical software system that conforms to stringent healthcare regulatory standards. We are practicing three styles of coordination practices that brings a solution to communication challenges. We are also describing our experiences of SAFe practices, Spotify like team culture, and 'Psychological Safety' that that helps in time-critical situations. The authors bring our experiences as a Program Manager, Project Manager, Quality Manager, and chief Architect who has been an integral part of the journey and establishing these practices over since the incubation stage of the referred product. These practices have helped to an extent where we have achieved regulatory acceptance and milestones successfully within aggressive time and taking steady steps towards where other business units are adopting our practices for managing multiple healthcare software systems.",
  "collection-title": "ICGSE '20",
  "container-title": "Proceedings of the 15th International Conference on Global Software Engineering",
  "DOI": "10.1145/3372787.3389300",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7093-6",
  "page": "6–10",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Challenges in scaling AI-powered distributed software product: a case study of a healthcare organization",
  "title-short": "Challenges in scaling AI-powered distributed software product",
  "URL": "https://doi.org/10.1145/3372787.3389300",
  "author": [
    {
      "family": "Gupta",
      "given": "Rajeev K"
    },
    {
      "family": "B",
      "given": "Balaji"
    },
    {
      "family": "V",
      "given": "Mekanathan"
    },
    {
      "family": "J",
      "given": "Ferose Khan"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b236f"
  },
  "type": "paper-conference",
  "abstract": "Autonomous vehicles (AVs) are increasingly becoming part of the emerging Intelligent Transportation Systems (ITS) and they are positioned to advance smart mobility. To enable this, new on-board sensors collect and transmit growing types and quantities of data. This raises new and unique privacy considerations around what happens with this data. As the automotive industry becomes more data-driven, getting consumer privacy rights will become increasingly important for establishing trust and customer acceptance of this technology. At the same time, the algorithmic decision making in AVs raises several new ethical issues that can create new safety risks and discriminatory outcomes. In this paper we analyze what are the new privacy and data protection challenges that emerge in AVs and investigate the ethical and liability concerns surrounding algorithmic decision-making, highlighting research gaps and the need to mitigate these issues by acting swiftly.",
  "collection-title": "CSCS '20",
  "container-title": "Computer Science in Cars Symposium",
  "DOI": "10.1145/3385958.3430481",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7621-1",
  "page": "1–10",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Autonomous Vehicles: Data Protection and Ethical Considerations",
  "title-short": "Autonomous Vehicles",
  "URL": "https://doi.org/10.1145/3385958.3430481",
  "author": [
    {
      "family": "Krontiris",
      "given": "Ioannis"
    },
    {
      "family": "Grammenou",
      "given": "Kalliroi"
    },
    {
      "family": "Terzidou",
      "given": "Kalliopi"
    },
    {
      "family": "Zacharopoulou",
      "given": "Marina"
    },
    {
      "family": "Tsikintikou",
      "given": "Marina"
    },
    {
      "family": "Baladima",
      "given": "Foteini"
    },
    {
      "family": "Sakellari",
      "given": "Chrysi"
    },
    {
      "family": "Kaouras",
      "given": "Konstantinos"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2370"
  },
  "type": "paper-conference",
  "abstract": "With the ever-increasing growth of online recruitment data, job-resume matching has become an important task to automatically match jobs with suitable resumes. This task is typically casted as a supervised text matching problem. Supervised learning is powerful when the labeled data is sufficient. However, on online recruitment platforms, job-resume interaction data is sparse and noisy, which affects the performance of job-resume match algorithms. To alleviate these problems, in this paper, we propose a novel multi-view co-teaching network from sparse interaction data for job-resume matching. Our network consists of two major components, namely text-based matching model and relation-based matching model. The two parts capture semantic compatibility in two different views, and complement each other. In order to ad- dress the challenges from sparse and noisy data, we design two specific strategies to combine the two components. First, two com- ponents share the learned parameters or representations, so that the original representations of each component can be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the influence of noise in training data. The core idea is to let the two components help each other by selecting more reliable training instances. The two strategies focus on representation enhancement and data enhancement, respectively. Compared with pure text-based matching models, the proposed approach is able to learn better data representations from limited or even sparse interaction data, which is more resistible to noise in training data. Experiment results have demonstrated that our model is able to outperform state-of-the-art methods for job-resume matching.",
  "collection-title": "CIKM '20",
  "container-title": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management",
  "DOI": "10.1145/3340531.3411929",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6859-9",
  "page": "65–74",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Learning to Match Jobs with Resumes from Sparse Interaction Data using Multi-View Co-Teaching Network",
  "URL": "https://doi.org/10.1145/3340531.3411929",
  "author": [
    {
      "family": "Bian",
      "given": "Shuqing"
    },
    {
      "family": "Chen",
      "given": "Xu"
    },
    {
      "family": "Zhao",
      "given": "Wayne Xin"
    },
    {
      "family": "Zhou",
      "given": "Kun"
    },
    {
      "family": "Hou",
      "given": "Yupeng"
    },
    {
      "family": "Song",
      "given": "Yang"
    },
    {
      "family": "Zhang",
      "given": "Tao"
    },
    {
      "family": "Wen",
      "given": "Ji-Rong"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2371"
  },
  "type": "paper-conference",
  "abstract": "Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.",
  "collection-title": "ICSE '20",
  "container-title": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
  "DOI": "10.1145/3377811.3380415",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7121-6",
  "page": "1147–1158",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Fuzz testing based data augmentation to improve robustness of deep neural networks",
  "URL": "https://doi.org/10.1145/3377811.3380415",
  "author": [
    {
      "family": "Gao",
      "given": "Xiang"
    },
    {
      "family": "Saha",
      "given": "Ripon K."
    },
    {
      "family": "Prasad",
      "given": "Mukul R."
    },
    {
      "family": "Roychoudhury",
      "given": "Abhik"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Testing",
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2372"
  },
  "type": "paper-conference",
  "abstract": "Program slicing has been widely applied in a variety of software engineering tasks. However, existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables, rather than neural networks that are composed of neurons and synapses. In this paper, we introduce NNSlicer, the first approach for slicing deep neural networks based on data-flow analysis. Our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset. Then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons, and calculate the slice as the neurons and the synapses with larger contributions. We demonstrate the usefulness and effectiveness of NNSlicer with three applications, including adversarial input detection, model pruning, and selective model protection. In all applications, NNSlicer significantly outperforms other baselines that do not rely on data flow analysis.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3409676",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "838–850",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Dynamic slicing for deep neural networks",
  "URL": "https://doi.org/10.1145/3368089.3409676",
  "author": [
    {
      "family": "Zhang",
      "given": "Ziqi"
    },
    {
      "family": "Li",
      "given": "Yuanchun"
    },
    {
      "family": "Guo",
      "given": "Yao"
    },
    {
      "family": "Chen",
      "given": "Xiangqun"
    },
    {
      "family": "Liu",
      "given": "Yunxin"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2373"
  },
  "type": "paper-conference",
  "abstract": "The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow. DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps. This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.",
  "collection-title": "ICSSP '20",
  "container-title": "Proceedings of the International Conference on Software and System Processes",
  "DOI": "10.1145/3379177.3388909",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7512-2",
  "page": "165–174",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "From Ad-Hoc Data Analytics to DataOps",
  "URL": "https://doi.org/10.1145/3379177.3388909",
  "author": [
    {
      "family": "Munappy",
      "given": "Aiswarya Raj"
    },
    {
      "family": "Mattos",
      "given": "David Issa"
    },
    {
      "family": "Bosch",
      "given": "Jan"
    },
    {
      "family": "Olsson",
      "given": "Helena Holmström"
    },
    {
      "family": "Dakkak",
      "given": "Anas"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2374"
  },
  "type": "paper-conference",
  "abstract": "AI models and services are used in a growing number of high-stakes areas, resulting in a need for increased transparency. Consistent with this, several proposals for higher quality and more consistent documentation of AI data, models, and systems have emerged. Little is known, however, about the needs of those who would produce or consume these new forms of documentation. Through semi-structured developer interviews, and two document-creation exercises, we have assembled a clearer picture of these needs and the various challenges faced in creating accurate and useful AI documentation. Based on the observations from this work, supplemented by feedback received during multiple design explorations and stakeholder conversations, we make recommendations for easing the collection and flexible presentation of AI facts to promote transparency.",
  "collection-title": "CHI EA '20",
  "container-title": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems",
  "DOI": "10.1145/3334480.3383051",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6819-3",
  "page": "1–8",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Experiences with Improving the Transparency of AI Models and Services",
  "URL": "https://doi.org/10.1145/3334480.3383051",
  "author": [
    {
      "family": "Hind",
      "given": "Michael"
    },
    {
      "family": "Houde",
      "given": "Stephanie"
    },
    {
      "family": "Martino",
      "given": "Jacquelyn"
    },
    {
      "family": "Mojsilovic",
      "given": "Aleksandra"
    },
    {
      "family": "Piorkowski",
      "given": "David"
    },
    {
      "family": "Richards",
      "given": "John"
    },
    {
      "family": "Varshney",
      "given": "Kush R."
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "AI Engineering",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2375"
  },
  "type": "paper-conference",
  "abstract": "Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners' needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We highlight aspects of organizational culture that may impact the efficacy of AI fairness checklists, and suggest future design directions.",
  "collection-title": "CHI '20",
  "container-title": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "DOI": "10.1145/3313831.3376445",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6708-0",
  "page": "1–14",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI",
  "URL": "https://doi.org/10.1145/3313831.3376445",
  "author": [
    {
      "family": "Madaio",
      "given": "Michael A."
    },
    {
      "family": "Stark",
      "given": "Luke"
    },
    {
      "family": "Wortman Vaughan",
      "given": "Jennifer"
    },
    {
      "family": "Wallach",
      "given": "Hanna"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2376"
  },
  "type": "paper-conference",
  "abstract": "Robots that support humans by performing useful tasks (a.k.a., service robots) are booming worldwide. In contrast to industrial robots, the development of service robots comes with severe software engineering challenges, since they require high levels of robustness and autonomy to operate in highly heterogeneous environments. As a domain with critical safety implications, service robotics faces a need for sound software development practices. In this paper, we present the first large-scale empirical study to assess the state of the art and practice of robotics software engineering. We conducted 18 semi-structured interviews with industrial practitioners working in 15 companies from 9 different countries and a survey with 156 respondents from 26 countries from the robotics domain. Our results provide a comprehensive picture of (i) the practices applied by robotics industrial and academic practitioners, including processes, paradigms, languages, tools, frameworks, and reuse practices, (ii) the distinguishing characteristics of robotics software engineering, and (iii) recurrent challenges usually faced, together with adopted solutions. The paper concludes by discussing observations, derived hypotheses, and proposed actions for researchers and practitioners.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3409743",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "593–604",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Robotics software engineering: a perspective from the service robotics domain",
  "title-short": "Robotics software engineering",
  "URL": "https://doi.org/10.1145/3368089.3409743",
  "author": [
    {
      "family": "García",
      "given": "Sergio"
    },
    {
      "family": "Strüber",
      "given": "Daniel"
    },
    {
      "family": "Brugali",
      "given": "Davide"
    },
    {
      "family": "Berger",
      "given": "Thorsten"
    },
    {
      "family": "Pelliccione",
      "given": "Patrizio"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2377"
  },
  "type": "paper-conference",
  "abstract": "In this experience report, we discuss the development of a solution that enables conflict-affected youth to discover and access relevant learning content. A team of individuals from a not-for-profit, a large multi-national technology company, and an academic institution, collaborated to develop that solution as a conversational agent named Hakeem. We provide a brief motivation and product description before outlining our design and development process including forming a distributed virtual team, engaging in user-centred design with conflict-affected youth in Lebanon, and using a minimum viable product approach while adapting Scrum for distributed development. We end this report with a reflection on the lessons learned thus far.",
  "collection-title": "ICGSE '20",
  "container-title": "Proceedings of the 15th International Conference on Global Software Engineering",
  "DOI": "10.1145/3372787.3390430",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7093-6",
  "page": "122–126",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Developing a conversational agent with a globally distributed team: an experience report",
  "title-short": "Developing a conversational agent with a globally distributed team",
  "URL": "https://doi.org/10.1145/3372787.3390430",
  "author": [
    {
      "family": "Ruane",
      "given": "Elayne"
    },
    {
      "family": "Smith",
      "given": "Ross"
    },
    {
      "family": "Bean",
      "given": "Dan"
    },
    {
      "family": "Tjalve",
      "given": "Michael"
    },
    {
      "family": "Ventresque",
      "given": "Anthony"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2378"
  },
  "type": "paper-conference",
  "abstract": "Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.",
  "collection-title": "SPLC '20",
  "container-title": "Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A",
  "DOI": "10.1145/3382025.3414968",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7569-6",
  "page": "1–11",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss",
  "title-short": "Automated extraction of domain knowledge in practice",
  "URL": "https://doi.org/10.1145/3382025.3414968",
  "author": [
    {
      "family": "Li",
      "given": "Yang"
    },
    {
      "family": "Schulze",
      "given": "Sandro"
    },
    {
      "family": "Scherrebeck",
      "given": "Helene Hvidegaard"
    },
    {
      "family": "Fogdal",
      "given": "Thomas Sorensen"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2379"
  },
  "type": "paper-conference",
  "abstract": "Deep Neural Networks (DNNs) are rapidly being adopted by the automotive industry, due to their impressive performance in tasks that are essential for autonomous driving. Object segmentation is one such task: its aim is to precisely locate boundaries of objects and classify the identified objects, helping autonomous cars to recognise the road environment and the traffic situation. Not only is this task safety critical, but developing a DNN based object segmentation module presents a set of challenges that are significantly different from traditional development of safety critical software. The development process in use consists of multiple iterations of data collection, labelling, training, and evaluation. Among these stages, training and evaluation are computation intensive while data collection and labelling are manual labour intensive. This paper shows how development of DNN based object segmentation can be improved by exploiting the correlation between Surprise Adequacy (SA) and model performance. The correlation allows us to predict model performance for inputs without manually labelling them. This, in turn, enables understanding of model performance, more guided data collection, and informed decisions about further training. In our industrial case study the technique allows cost savings of up to 50% with negligible evaluation inaccuracy. Furthermore, engineers can trade off cost savings versus the tolerable level of inaccuracy depending on different development phases and scenarios.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3417065",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "1466–1476",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Reducing DNN labelling cost using surprise adequacy: an industrial case study for autonomous driving",
  "title-short": "Reducing DNN labelling cost using surprise adequacy",
  "URL": "https://doi.org/10.1145/3368089.3417065",
  "author": [
    {
      "family": "Kim",
      "given": "Jinhan"
    },
    {
      "family": "Ju",
      "given": "Jeongil"
    },
    {
      "family": "Feldt",
      "given": "Robert"
    },
    {
      "family": "Yoo",
      "given": "Shin"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development",
    "Data Management",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237a"
  },
  "type": "paper-conference",
  "abstract": "For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover’sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8% improvement on cluster quality, reducing 65.9% number of clusters, i.e., the number of test API methods to be implemented.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3417067",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "1285–1295",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Clustering test steps in natural language toward automating test automation",
  "URL": "https://doi.org/10.1145/3368089.3417067",
  "author": [
    {
      "family": "Li",
      "given": "Linyi"
    },
    {
      "family": "Li",
      "given": "Zhenwen"
    },
    {
      "family": "Zhang",
      "given": "Weijie"
    },
    {
      "family": "Zhou",
      "given": "Jun"
    },
    {
      "family": "Wang",
      "given": "Pengcheng"
    },
    {
      "family": "Wu",
      "given": "Jing"
    },
    {
      "family": "He",
      "given": "Guanghua"
    },
    {
      "family": "Zeng",
      "given": "Xia"
    },
    {
      "family": "Deng",
      "given": "Yuetang"
    },
    {
      "family": "Xie",
      "given": "Tao"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237b"
  },
  "type": "paper-conference",
  "abstract": "In many software and systems development projects, analysts specify requirements using a combination of modeling and natural language (NL). In such situations, systematic acceptance testing poses a challenge because defining the acceptance criteria (AC) to be met by the system under test has to account not only for the information in the (requirements) model but also that in the NL requirements. In other words, neither models nor NL requirements per se provide a complete picture of the information content relevant to AC. Our work in this paper is prompted by the observation that a reconciliation of the information content in NL requirements and models is necessary for obtaining precise AC. We perform such reconciliation by devising an approach that automatically extracts AC-related information from NL requirements and helps modelers enrich their model with the extracted information. An existing AC derivation technique is then applied to the model that has now been enriched by the information extracted from NL requirements. Using a real case study from the financial domain, we evaluate the usefulness of the AC-related model enrichments recommended by our approach. Our evaluation results are very promising: Over our case study system, a group of five domain experts found 89% of the recommended enrichments relevant to AC and yet absent from the original model (precision of 89%). Furthermore, the experts could not pinpoint any additional information in the NL requirements which was relevant to AC but which had not already been brought to their attention by our approach (recall of 100%).",
  "collection-title": "MODELS '20",
  "container-title": "Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems",
  "DOI": "10.1145/3365438.3410953",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7019-6",
  "page": "218–228",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Leveraging natural-language requirements for deriving better acceptance criteria from models",
  "URL": "https://doi.org/10.1145/3365438.3410953",
  "author": [
    {
      "family": "Veizaga",
      "given": "Alvaro"
    },
    {
      "family": "Alferez",
      "given": "Mauricio"
    },
    {
      "family": "Torre",
      "given": "Damiano"
    },
    {
      "family": "Sabetzadeh",
      "given": "Mehrdad"
    },
    {
      "family": "Briand",
      "given": "Lionel"
    },
    {
      "family": "Pitskhelauri",
      "given": "Elene"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Testing",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237c"
  },
  "type": "paper-conference",
  "abstract": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.",
  "collection-title": "CHI '20",
  "container-title": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "DOI": "10.1145/3313831.3376590",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6708-0",
  "page": "1–15",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Questioning the AI: Informing Design Practices for Explainable AI User Experiences",
  "title-short": "Questioning the AI",
  "URL": "https://doi.org/10.1145/3313831.3376590",
  "author": [
    {
      "family": "Liao",
      "given": "Q. Vera"
    },
    {
      "family": "Gruen",
      "given": "Daniel"
    },
    {
      "family": "Miller",
      "given": "Sarah"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237d"
  },
  "type": "paper-conference",
  "abstract": "Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.",
  "collection-title": "ESEM '20",
  "container-title": "Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
  "DOI": "10.1145/3382494.3410681",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7580-1",
  "page": "1–12",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Adoption and Effects of Software Engineering Best Practices in Machine Learning",
  "URL": "https://doi.org/10.1145/3382494.3410681",
  "author": [
    {
      "family": "Serban",
      "given": "Alex"
    },
    {
      "family": "Blom",
      "given": "Koen",
      "non-dropping-particle": "van der"
    },
    {
      "family": "Hoos",
      "given": "Holger"
    },
    {
      "family": "Visser",
      "given": "Joost"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality",
    "Model Development",
    "Model Deployment",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237e"
  },
  "type": "paper-conference",
  "abstract": "Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.",
  "collection-title": "ICSE '20",
  "container-title": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
  "DOI": "10.1145/3377811.3380403",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7121-6",
  "page": "554–565",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "An investigation of cross-project learning in online just-in-time software defect prediction",
  "URL": "https://doi.org/10.1145/3377811.3380403",
  "author": [
    {
      "family": "Tabassum",
      "given": "Sadia"
    },
    {
      "family": "Minku",
      "given": "Leandro L."
    },
    {
      "family": "Feng",
      "given": "Danyi"
    },
    {
      "family": "Cabral",
      "given": "George G."
    },
    {
      "family": "Song",
      "given": "Liyan"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b237f"
  },
  "type": "paper-conference",
  "abstract": "MLModelCI provides multimedia researchers and developers with a one-stop platform for efficient machine learning (ML) services. The system leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services (MLaaS). In its essence, MLModelCI serves as a housekeeper to help users publish models. The models are first automatically converted to optimized formats for production purpose and then profiled under different settings (e.g., batch size and hardware). The profiling information can be used as guidelines for balancing the trade-off between performance and cost of MLaaS. Finally, the system dockerizes the models for ease of deployment to cloud environments. A key feature of MLModelCI is the implementation of a controller, which allows elastic evaluation which only utilizes idle workers while maintaining online service quality. Our system bridges the gap between current ML training and serving systems and thus free developers from manual and tedious work often associated with service deployment. We release the platform as an open-source project on GitHub under Apache 2.0 license, with the aim that it will facilitate and streamline more large-scale ML applications and research projects.",
  "collection-title": "MM '20",
  "container-title": "Proceedings of the 28th ACM International Conference on Multimedia",
  "DOI": "10.1145/3394171.3414535",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7988-5",
  "page": "4453–4456",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "MLModelCI: An Automatic Cloud Platform for Efficient MLaaS",
  "title-short": "MLModelCI",
  "URL": "https://doi.org/10.1145/3394171.3414535",
  "author": [
    {
      "family": "Zhang",
      "given": "Huaizheng"
    },
    {
      "family": "Li",
      "given": "Yuanming"
    },
    {
      "family": "Huang",
      "given": "Yizheng"
    },
    {
      "family": "Wen",
      "given": "Yonggang"
    },
    {
      "family": "Yin",
      "given": "Jianxiong"
    },
    {
      "family": "Guan",
      "given": "Kyle"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Infrastructure",
    "AI Software Quality",
    "Model Deployment",
    "Testing",
    "Model Development",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2380"
  },
  "type": "paper-conference",
  "abstract": "Successful machine learning (ML) applications require iterations on both modeling and the underlying data. While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models. We also identify common types of data iterations and associated analysis tasks and challenges. To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions. We present two case studies where developers apply \\system to their own evolving datasets on production ML projects. Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.",
  "collection-title": "CHI '20",
  "container-title": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
  "DOI": "10.1145/3313831.3376177",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6708-0",
  "page": "1–13",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Understanding and Visualizing Data Iteration in Machine Learning",
  "URL": "https://doi.org/10.1145/3313831.3376177",
  "author": [
    {
      "family": "Hohman",
      "given": "Fred"
    },
    {
      "family": "Wongsuphasawat",
      "given": "Kanit"
    },
    {
      "family": "Kery",
      "given": "Mary Beth"
    },
    {
      "family": "Patel",
      "given": "Kayur"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development",
    "AI Software Quality",
    "Data Management"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2381"
  },
  "type": "paper-conference",
  "abstract": "Deep neural networks (DNN) have been deployed in many software systems to assist in various classification tasks. In company with the fantastic effectiveness in classification, DNNs could also exhibit incorrect behaviors and result in accidents and losses. Therefore, testing techniques that can detect incorrect DNN behaviors and improve DNN quality are extremely necessary and critical. However, the testing oracle, which defines the correct output for a given input, is often not available in the automated testing. To obtain the oracle information, the testing tasks of DNN-based systems usually require expensive human efforts to label the testing data, which significantly slows down the process of quality assurance. To mitigate this problem, we propose DeepGini, a test prioritization technique designed based on a statistical perspective of DNN. Such a statistical perspective allows us to reduce the problem of measuring misclassification probability to the problem of measuring set impurity, which allows us to quickly identify possibly-misclassified tests. To evaluate, we conduct an extensive empirical study on popular datasets and prevalent DNN models. The experimental results demonstrate that DeepGini outperforms existing coverage-based techniques in prioritizing tests regarding both effectiveness and efficiency. Meanwhile, we observe that the tests prioritized at the front by DeepGini are more effective in improving the DNN quality in comparison with the coverage-based techniques.",
  "collection-title": "ISSTA 2020",
  "container-title": "Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis",
  "DOI": "10.1145/3395363.3397357",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-8008-9",
  "page": "177–188",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "DeepGini: prioritizing massive tests to enhance the robustness of deep neural networks",
  "title-short": "DeepGini",
  "URL": "https://doi.org/10.1145/3395363.3397357",
  "author": [
    {
      "family": "Feng",
      "given": "Yang"
    },
    {
      "family": "Shi",
      "given": "Qingkai"
    },
    {
      "family": "Gao",
      "given": "Xinyu"
    },
    {
      "family": "Wan",
      "given": "Jun"
    },
    {
      "family": "Fang",
      "given": "Chunrong"
    },
    {
      "family": "Chen",
      "given": "Zhenyu"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management",
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2382"
  },
  "type": "paper-conference",
  "abstract": "Deep learning (DL) has become one of the most successful machine learning techniques. To achieve the optimal development result, there are emerging requirements on the interoperability between DL frameworks that the trained model files and training/serving programs can be re-utilized. Faithful model conversion is a promising technology to enhance the framework interoperability in which a source model is transformed into the semantic equivalent in another target framework format. However, several major challenges need to be addressed. First, there are apparent discrepancies between DL frameworks. Second, understanding the semantics of a source model could be difficult due to the framework scheme and optimization. Lastly, there exist a large number of DL frameworks, bringing potential significant engineering efforts. In this paper, we propose MMdnn, an open-sourced, comprehensive, and faithful model conversion tool for popular DL frameworks. MMdnn adopts a novel unified intermediate representation (IR)-based methodology to systematically handle the conversion challenges. The source model is first transformed into an intermediate computation graph represented by the simple graph-based IR of MMdnn and then to the target framework format, which greatly reduces the engineering complexity. Since the model structure expressed by developers may have been changed by DL frameworks (e.g., graph optimization), MMdnn tries to recover the original high-level neural network layers for better semantic comprehension via a pattern matching similar method. In the meantime, a piece of model construction code is generated to facilitate later retraining or serving. MMdnn implements an extensible conversion architecture from the compilation point of view, which eases contribution from the community to support new DL operators and frameworks. MMdnn has reached good maturity and quality, and is applied for converting production models.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3417051",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "1320–1330",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Enhancing the interoperability between deep learning frameworks by model conversion",
  "URL": "https://doi.org/10.1145/3368089.3417051",
  "author": [
    {
      "family": "Liu",
      "given": "Yu"
    },
    {
      "family": "Chen",
      "given": "Cheng"
    },
    {
      "family": "Zhang",
      "given": "Ru"
    },
    {
      "family": "Qin",
      "given": "Tingting"
    },
    {
      "family": "Ji",
      "given": "Xiang"
    },
    {
      "family": "Lin",
      "given": "Haoxiang"
    },
    {
      "family": "Yang",
      "given": "Mao"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Infrastructure",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2383"
  },
  "type": "paper-conference",
  "abstract": "Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation. This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9%, the per-class accuracy difference to be up to 52.4%, and the training time difference to be up to 145.3%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions. Our researcher and practitioner survey shows that 83.8% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5±3% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.",
  "collection-title": "ASE '20",
  "container-title": "Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering",
  "DOI": "10.1145/3324884.3416545",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6768-4",
  "page": "771–783",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Problems and opportunities in training deep learning software systems: an analysis of variance",
  "title-short": "Problems and opportunities in training deep learning software systems",
  "URL": "https://doi.org/10.1145/3324884.3416545",
  "author": [
    {
      "family": "Pham",
      "given": "Hung Viet"
    },
    {
      "family": "Qian",
      "given": "Shangshu"
    },
    {
      "family": "Wang",
      "given": "Jiannan"
    },
    {
      "family": "Lutellier",
      "given": "Thibaud"
    },
    {
      "family": "Rosenthal",
      "given": "Jonathan"
    },
    {
      "family": "Tan",
      "given": "Lin"
    },
    {
      "family": "Yu",
      "given": "Yaoliang"
    },
    {
      "family": "Nagappan",
      "given": "Nachiappan"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development",
    "AI Software Quality",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2384"
  },
  "type": "paper-conference",
  "abstract": "Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL framework runtime. We have evaluated DNNMem on 5 real-world representative models with different hyperparameters under 3 mainstream frameworks (TensorFlow, PyTorch, and MXNet). Our extensive experiments show that DNNMem is effective in estimating GPU memory consumption.",
  "collection-title": "ESEC/FSE 2020",
  "container-title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3368089.3417050",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7043-1",
  "page": "1342–1352",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Estimating GPU memory consumption of deep learning models",
  "URL": "https://doi.org/10.1145/3368089.3417050",
  "author": [
    {
      "family": "Gao",
      "given": "Yanjie"
    },
    {
      "family": "Liu",
      "given": "Yu"
    },
    {
      "family": "Zhang",
      "given": "Hongyu"
    },
    {
      "family": "Li",
      "given": "Zhengxian"
    },
    {
      "family": "Zhu",
      "given": "Yonghao"
    },
    {
      "family": "Lin",
      "given": "Haoxiang"
    },
    {
      "family": "Yang",
      "given": "Mao"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2385"
  },
  "type": "paper-conference",
  "abstract": "We present the design of MTKeras, a generic metamorphic testing framework for machine learning, and demonstrate its effectiveness through case studies in image classification and sentiment analysis.",
  "collection-title": "ICSEW'20",
  "container-title": "Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops",
  "DOI": "10.1145/3387940.3392694",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7963-2",
  "page": "386–387",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "A Testing Tool for Machine Learning Applications",
  "URL": "https://doi.org/10.1145/3387940.3392694",
  "author": [
    {
      "family": "Liu",
      "given": "Yelin"
    },
    {
      "family": "Liu",
      "given": "Yang"
    },
    {
      "family": "Chen",
      "given": "Tsong Yueh"
    },
    {
      "family": "Zhou",
      "given": "Zhi Quan"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2386"
  },
  "type": "paper-conference",
  "abstract": "Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.",
  "collection-title": "FAT* '20",
  "container-title": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",
  "DOI": "10.1145/3351095.3372873",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6936-7",
  "page": "33–44",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing",
  "title-short": "Closing the AI accountability gap",
  "URL": "https://doi.org/10.1145/3351095.3372873",
  "author": [
    {
      "family": "Raji",
      "given": "Inioluwa Deborah"
    },
    {
      "family": "Smart",
      "given": "Andrew"
    },
    {
      "family": "White",
      "given": "Rebecca N."
    },
    {
      "family": "Mitchell",
      "given": "Margaret"
    },
    {
      "family": "Gebru",
      "given": "Timnit"
    },
    {
      "family": "Hutchinson",
      "given": "Ben"
    },
    {
      "family": "Smith-Loud",
      "given": "Jamila"
    },
    {
      "family": "Theron",
      "given": "Daniel"
    },
    {
      "family": "Barnes",
      "given": "Parker"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2387"
  },
  "type": "paper-conference",
  "abstract": "Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.",
  "container-title": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)",
  "DOI": "10.1109/ISCA45697.2020.00045",
  "event": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)",
  "page": "446-459",
  "source": "IEEE Xplore",
  "title": "MLPerf Inference Benchmark",
  "author": [
    {
      "family": "Reddi",
      "given": "Vijay Janapa"
    },
    {
      "family": "Cheng",
      "given": "Christine"
    },
    {
      "family": "Kanter",
      "given": "David"
    },
    {
      "family": "Mattson",
      "given": "Peter"
    },
    {
      "family": "Schmuelling",
      "given": "Guenther"
    },
    {
      "family": "Wu",
      "given": "Carole-Jean"
    },
    {
      "family": "Anderson",
      "given": "Brian"
    },
    {
      "family": "Breughe",
      "given": "Maximilien"
    },
    {
      "family": "Charlebois",
      "given": "Mark"
    },
    {
      "family": "Chou",
      "given": "William"
    },
    {
      "family": "Chukka",
      "given": "Ramesh"
    },
    {
      "family": "Coleman",
      "given": "Cody"
    },
    {
      "family": "Davis",
      "given": "Sam"
    },
    {
      "family": "Deng",
      "given": "Pan"
    },
    {
      "family": "Diamos",
      "given": "Greg"
    },
    {
      "family": "Duke",
      "given": "Jared"
    },
    {
      "family": "Fick",
      "given": "Dave"
    },
    {
      "family": "Gardner",
      "given": "J. Scott"
    },
    {
      "family": "Hubara",
      "given": "Itay"
    },
    {
      "family": "Idgunji",
      "given": "Sachin"
    },
    {
      "family": "Jablin",
      "given": "Thomas B."
    },
    {
      "family": "Jiao",
      "given": "Jeff"
    },
    {
      "family": "John",
      "given": "Tom St."
    },
    {
      "family": "Kanwar",
      "given": "Pankaj"
    },
    {
      "family": "Lee",
      "given": "David"
    },
    {
      "family": "Liao",
      "given": "Jeffery"
    },
    {
      "family": "Lokhmotov",
      "given": "Anton"
    },
    {
      "family": "Massa",
      "given": "Francisco"
    },
    {
      "family": "Meng",
      "given": "Peng"
    },
    {
      "family": "Micikevicius",
      "given": "Paulius"
    },
    {
      "family": "Osborne",
      "given": "Colin"
    },
    {
      "family": "Pekhimenko",
      "given": "Gennady"
    },
    {
      "family": "Rajan",
      "given": "Arun Tejusve Raghunath"
    },
    {
      "family": "Sequeira",
      "given": "Dilip"
    },
    {
      "family": "Sirasao",
      "given": "Ashish"
    },
    {
      "family": "Sun",
      "given": "Fei"
    },
    {
      "family": "Tang",
      "given": "Hanlin"
    },
    {
      "family": "Thomson",
      "given": "Michael"
    },
    {
      "family": "Wei",
      "given": "Frank"
    },
    {
      "family": "Wu",
      "given": "Ephrem"
    },
    {
      "family": "Xu",
      "given": "Lingjie"
    },
    {
      "family": "Yamada",
      "given": "Koichi"
    },
    {
      "family": "Yu",
      "given": "Bing"
    },
    {
      "family": "Yuan",
      "given": "George"
    },
    {
      "family": "Zhong",
      "given": "Aaron"
    },
    {
      "family": "Zhang",
      "given": "Peizhao"
    },
    {
      "family": "Zhou",
      "given": "Yuchen"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2388"
  },
  "type": "article-journal",
  "abstract": "Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",
  "container-title": "Proceedings of the ACM on Human-Computer Interaction",
  "DOI": "10.1145/3392826",
  "issue": "CSCW1",
  "journalAbbreviation": "Proc. ACM Hum.-Comput. Interact.",
  "page": "022:1–022:23",
  "source": "May 2020",
  "title": "How do Data Science Workers Collaborate? Roles, Workflows, and Tools",
  "title-short": "How do Data Science Workers Collaborate?",
  "URL": "https://doi.org/10.1145/3392826",
  "volume": "4",
  "author": [
    {
      "family": "Zhang",
      "given": "Amy X."
    },
    {
      "family": "Muller",
      "given": "Michael"
    },
    {
      "family": "Wang",
      "given": "Dakuo"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Project Management",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b2389"
  },
  "type": "paper-conference",
  "abstract": "The development of artificial intelligence, e. g., for Computer Vision, through supervised learning requires the input of large amounts of annotated or labeled data objects as training data. The creation of high-quality training data is usually done manually which can be repetitive and tiring. Gamification, the use of game elements in a non-game context, is one method to make tedious tasks more interesting. This paper proposes a multi-step process for gamifying the manual creation of training data for machine learning purposes. We choose a user-adapted approach based on the results of a preceding user study with the target group (employees of an AI software development company) which helped us to identify annotation use cases and the users' player characteristics. The resulting concept includes levels of increasing difficulty, tutorials, progress indicators and a narrative built around a robot character which at the same time is a user assistant. The implemented prototype is an extension of the company's existing annotation tool and serves as a basis for further observations.",
  "collection-title": "MuC '20",
  "container-title": "Proceedings of the Conference on Mensch und Computer",
  "DOI": "10.1145/3404983.3405519",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7540-5",
  "page": "173–181",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Achiever or explorer? gamifying the creation process of training data for machine learning",
  "title-short": "Achiever or explorer?",
  "URL": "https://doi.org/10.1145/3404983.3405519",
  "author": [
    {
      "family": "Alaghbari",
      "given": "Sarah"
    },
    {
      "family": "Mitschick",
      "given": "Annett"
    },
    {
      "family": "Blichmann",
      "given": "Gregor"
    },
    {
      "family": "Voigt",
      "given": "Martin"
    },
    {
      "family": "Dachselt",
      "given": "Raimund"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b238a"
  },
  "type": "paper-conference",
  "abstract": "To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---differential score and differential rank---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.",
  "collection-title": "CCS '20",
  "container-title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
  "DOI": "10.1145/3372297.3417880",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7089-9",
  "page": "363–375",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Analyzing Information Leakage of Updates to Natural Language Models",
  "URL": "https://doi.org/10.1145/3372297.3417880",
  "author": [
    {
      "family": "Zanella-Béguelin",
      "given": "Santiago"
    },
    {
      "family": "Wutschitz",
      "given": "Lukas"
    },
    {
      "family": "Tople",
      "given": "Shruti"
    },
    {
      "family": "Rühle",
      "given": "Victor"
    },
    {
      "family": "Paverd",
      "given": "Andrew"
    },
    {
      "family": "Ohrimenko",
      "given": "Olga"
    },
    {
      "family": "Köpf",
      "given": "Boris"
    },
    {
      "family": "Brockschmidt",
      "given": "Marc"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management",
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b238b"
  },
  "type": "paper-conference",
  "abstract": "As the use of machine learning techniques by organisations has become more common, the need for software tools that provide the robustness required in a production environment has become apparent. In this paper, we review relevant literature and outline a research agenda for the development of a low-code solution for monitoring the performance of a deployed machine learning model on a continuous basis.",
  "collection-title": "MODELS '20",
  "container-title": "Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings",
  "DOI": "10.1145/3417990.3420196",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-8135-2",
  "page": "1–8",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Towards a low-code solution for monitoring machine learning model performance",
  "URL": "https://doi.org/10.1145/3417990.3420196",
  "author": [
    {
      "family": "Kourouklidis",
      "given": "Panagiotis"
    },
    {
      "family": "Kolovos",
      "given": "Dimitris"
    },
    {
      "family": "Matragkas",
      "given": "Nicholas"
    },
    {
      "family": "Noppen",
      "given": "Joost"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Software Quality",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b238c"
  },
  "type": "paper-conference",
  "abstract": "As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of \"pipeline jungles\" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.",
  "collection-title": "SIGMOD '20",
  "container-title": "Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data",
  "DOI": "10.1145/3318464.3386146",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6735-6",
  "page": "785–800",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development",
  "title-short": "The Machine Learning Bazaar",
  "URL": "https://doi.org/10.1145/3318464.3386146",
  "author": [
    {
      "family": "Smith",
      "given": "Micah J."
    },
    {
      "family": "Sala",
      "given": "Carles"
    },
    {
      "family": "Kanter",
      "given": "James Max"
    },
    {
      "family": "Veeramachaneni",
      "given": "Kalyan"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "AI Engineering",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db004ab3da9102496b238d"
  },
  "type": "paper-conference",
  "abstract": "This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and \"upstream\" models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities.",
  "collection-title": "ICSEW'20",
  "container-title": "Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops",
  "DOI": "10.1145/3387940.3391496",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-7963-2",
  "page": "86–92",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Sensemaking Practices in the Everyday Work of AI/ML Software Engineering",
  "URL": "https://doi.org/10.1145/3387940.3391496",
  "author": [
    {
      "family": "Wolf",
      "given": "Christine T."
    },
    {
      "family": "Paine",
      "given": "Drew"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Model Development",
    "Project Management",
    "Requirement Engineering"
  ],
  "sol": "1"
},{
  "_id": {
    "$oid": "60db004ab3da9102496b238e"
  },
  "type": "paper-conference",
  "abstract": "Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.",
  "collection-title": "SBQS'20",
  "container-title": "19th Brazilian Symposium on Software Quality",
  "DOI": "10.1145/3439961.3439971",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-8923-5",
  "page": "1–10",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development",
  "title-short": "Brazilian Data Scientists",
  "URL": "https://doi.org/10.1145/3439961.3439971",
  "author": [
    {
      "family": "Correia",
      "given": "João Lucas"
    },
    {
      "family": "Pereira",
      "given": "Juliana Alves"
    },
    {
      "family": "Mello",
      "given": "Rafael"
    },
    {
      "family": "Garcia",
      "given": "Alessandro"
    },
    {
      "family": "Fonseca",
      "given": "Baldoino"
    },
    {
      "family": "Ribeiro",
      "given": "Márcio"
    },
    {
      "family": "Gheyi",
      "given": "Rohit"
    },
    {
      "family": "Kalinowski",
      "given": "Marcos"
    },
    {
      "family": "Cerqueira",
      "given": "Renato"
    },
    {
      "family": "Tiengo",
      "given": "Willy"
    }
  ],
  "year": "2020",
  "DB": "ACM",
  "categorie": [
    "Data Management",
    "Model Development",
    "Project Management",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db0fc3b3da9102496b238f"
  },
  "type": "article-journal",
  "abstract": "The Digital Age, the “Society 5.0” or the “4th Industrial Revolution” has created a challenging and evolving environment in which more up‐to‐date, secure, safer, cost‐efficient and personalized products and services must be timely delivered. Furthermore, the growing interest in equipping systems with intelligence implies that the engineering process must be adapted to consider the specific characteristics of Artificial Intelligent (AI) and Machine Learning (ML) systems. From the AI/ML point of view, the possibility of following an engineering process must also imply an improvement to overcome their “hidden” technical debt. To pave the way to the development of the next generation of smart systems, a retrospective in the current engineering practice and, in the development of AI/ML systems, is presented. Afterwards, the main challenges to harmonize both disciplines are outlined to finally describe the main opportunities and expected impacts.",
  "container-title": "INCOSE International Symposium",
  "DOI": "10.1002/j.2334-5837.2019.00621.x",
  "journalAbbreviation": "INCOSE International Symposium",
  "page": "560-575",
  "source": "ResearchGate",
  "title": "Challenges and opportunities in the integration of the Systems Engineering process and the AI/ML model lifecycle",
  "volume": "29",
  "author": [
    {
      "family": "Alvarez-Rodríguez",
      "given": "Jose"
    },
    {
      "family": "Zuñiga",
      "given": "Roy"
    },
    {
      "family": "Moreno",
      "given": "Valentín"
    },
    {
      "family": "Llorens",
      "given": "Juan"
    }
  ],
  "year": "2019",
  "DB": "Giray",
  "categorie": [
    "Requirement Engineering",
    "Infrastructure",
    "Testing",
    "AI Engineering",
    "Project Management",
    "Data Management",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db0fc3b3da9102496b2390"
  },
  "type": "paper-conference",
  "abstract": "We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.",
  "collection-title": "ISSTA 2018",
  "container-title": "Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis",
  "DOI": "10.1145/3213846.3213858",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5699-2",
  "page": "118–128",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Identifying implementation bugs in machine learning based image classifiers using metamorphic testing",
  "URL": "https://doi.org/10.1145/3213846.3213858",
  "author": [
    {
      "family": "Dwarakanath",
      "given": "Anurag"
    },
    {
      "family": "Ahuja",
      "given": "Manish"
    },
    {
      "family": "Sikand",
      "given": "Samarth"
    },
    {
      "family": "Rao",
      "given": "Raghotham M."
    },
    {
      "family": "Bose",
      "given": "R. P. Jagadeesh Chandra"
    },
    {
      "family": "Dubash",
      "given": "Neville"
    },
    {
      "family": "Podder",
      "given": "Sanjay"
    }
  ],
  "year": "2018",
  "DB": "Giray",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db0fc3b3da9102496b2391"
  },
  "type": "paper-conference",
  "abstract": "Automated vehicles are not supposed to fail at any time or in any situations during driving. Thus, vehicle manufactures and proving ground operators are challenged to complement existing test procedures with means to systematically evaluate automated driving. In this paper, we explore software related challenges from testing the safety of automated vehicles. We report on findings from conducting focus groups and interviews including 26 participants (e.g., vehicle manufacturers, suppliers, and researchers) from five countries.",
  "container-title": "2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)",
  "DOI": "10.1109/ICSE-C.2017.67",
  "event": "2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)",
  "page": "328-330",
  "source": "IEEE Xplore",
  "title": "Software-Related Challenges of Testing Automated Vehicles",
  "author": [
    {
      "family": "Knauss",
      "given": "A."
    },
    {
      "family": "Schroder",
      "given": "J."
    },
    {
      "family": "Berger",
      "given": "C."
    },
    {
      "family": "Eriksson",
      "given": "H."
    }
  ],
  "year": "2017",
  "DB": "Giray",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db0fc3b3da9102496b2392"
  },
  "type": "article-journal",
  "abstract": "Adding an ability for a system to learn inherently adds non-determinism into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work features (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.",
  "container-title": "IEEE Transactions on Software Engineering",
  "DOI": "10.1109/TSE.2019.2937083",
  "ISSN": "1939-3520",
  "note": "event: IEEE Transactions on Software Engineering",
  "page": "1-1",
  "source": "IEEE Xplore",
  "title": "How does Machine Learning Change Software Development Practices?",
  "author": [
    {
      "family": "Wan",
      "given": "Z."
    },
    {
      "family": "Xia",
      "given": "X."
    },
    {
      "family": "Lo",
      "given": "D."
    },
    {
      "family": "Murphy",
      "given": "G. C."
    }
  ],
  "year": "2019",
  "DB": "Giray",
  "categorie": [
    "Requirement Engineering",
    "Architecture Design",
    "Project Management",
    "AI Software Quality",
    "Testing",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2393"
  },
  "type": "paper-conference",
  "abstract": "Background: The number of Machine Learning (ML) systems developed in the industry is increasing rapidly. Since ML systems are different from traditional systems, these differences are clearly visible in different activities pertaining to ML systems software development process. These differences make the Software Engineering (SE) activities more challenging for ML systems because not only the behavior of the system is data dependent, but also the requirements are data dependent. In such scenario, how can Software Engineering better support the development of ML systems? Aim: Our objective is twofold. First, better understand the process that developers use to build ML systems. Second, identify the main challenges that developers face, proposing ways to overcome these challenges. Method: We conducted interviews with seven developers from three software small companies that develop ML systems. Based on the challenges uncovered, we proposed a set of checklists to support the developers. We assessed the checklists by using a focus group. Results: We found that the ML systems development follow a 4-stage process in these companies. These stages are: understanding the problem, data handling, model building, and model monitoring. The main challenges faced by the developers are: identifying the clients' business metrics, lack of a defined development process, and designing the database structure. We have identified in the focus group that our proposed checklists provided support during identification of the client's business metrics and in increasing visibility of the progress of the project tasks. Conclusions: Our research is an initial step towards supporting the development of ML systems, suggesting checklists that support developers in essential development tasks, and also serve as a basis for future research in the area.",
  "container-title": "2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
  "DOI": "10.1109/ESEM.2019.8870157",
  "event": "2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
  "note": "ISSN: 1949-3789",
  "page": "1-6",
  "source": "IEEE Xplore",
  "title": "Understanding Development Process of Machine Learning Systems: Challenges and Solutions",
  "title-short": "Understanding Development Process of Machine Learning Systems",
  "author": [
    {
      "family": "Nascimento",
      "given": "E. d S."
    },
    {
      "family": "Ahmed",
      "given": "I."
    },
    {
      "family": "Oliveira",
      "given": "E."
    },
    {
      "family": "Palheta",
      "given": "M. P."
    },
    {
      "family": "Steinmacher",
      "given": "I."
    },
    {
      "family": "Conte",
      "given": "T."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "AI Engineering",
    "Data Management",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2394"
  },
  "type": "paper-conference",
  "abstract": "Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",
  "container-title": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
  "DOI": "10.1109/ICSE-SEIP.2019.00042",
  "event": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
  "page": "291-300",
  "source": "IEEE Xplore",
  "title": "Software Engineering for Machine Learning: A Case Study",
  "title-short": "Software Engineering for Machine Learning",
  "author": [
    {
      "family": "Amershi",
      "given": "S."
    },
    {
      "family": "Begel",
      "given": "A."
    },
    {
      "family": "Bird",
      "given": "C."
    },
    {
      "family": "DeLine",
      "given": "R."
    },
    {
      "family": "Gall",
      "given": "H."
    },
    {
      "family": "Kamar",
      "given": "E."
    },
    {
      "family": "Nagappan",
      "given": "N."
    },
    {
      "family": "Nushi",
      "given": "B."
    },
    {
      "family": "Zimmermann",
      "given": "T."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Project Management",
    "Model Development",
    "Infrastructure",
    "Integration",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2395"
  },
  "type": "paper-conference",
  "abstract": "There is increasing interest in machine learning (ML) techniques and their applications in recent years. Although there has been intensive support by frameworks and libraries for the implementation of ML-based systems, investigation into engineering disciplines and methods is still at the early phase. The most pressing issue in this field is identifying the essential challenges for the software engineering research community as engineering of ML-based systems requires novel approaches due to the essentially different nature of ML-based systems. In this paper, we analyze the results of a questionnaire administered to 278 people who have worked on ML-based systems in practice, clarify the essential difficulties and their causes as perceived by practitioners, and suggest potential research directions.",
  "container-title": "2019 IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in Industry (CESI) and 6th International Workshop on Software Engineering Research and Industrial Practice (SER IP)",
  "DOI": "10.1109/CESSER-IP.2019.00009",
  "event": "2019 IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in Industry (CESI) and 6th International Workshop on Software Engineering Research and Industrial Practice (SER IP)",
  "note": "ISSN: 2575-4793",
  "page": "2-9",
  "source": "IEEE Xplore",
  "title": "How Do Engineers Perceive Difficulties in Engineering of Machine-Learning Systems? - Questionnaire Survey",
  "title-short": "How Do Engineers Perceive Difficulties in Engineering of Machine-Learning Systems?",
  "author": [
    {
      "family": "Ishikawa",
      "given": "F."
    },
    {
      "family": "Yoshioka",
      "given": "N."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "AI Engineering",
    "Project Management",
    "Model Development",
    "Testing",
    "Requirement Engineering",
    "Architecture Design"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2396"
  },
  "type": "article-journal",
  "abstract": "Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design.",
  "container-title": "arXiv:1906.12307 [cs]",
  "DOI": "10.1007/978-3-030-35333-9_24",
  "note": "arXiv: 1906.12307",
  "page": "331-338",
  "source": "arXiv.org",
  "title": "Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study",
  "title-short": "Implementing Ethics in AI",
  "URL": "http://arxiv.org/abs/1906.12307",
  "volume": "11915",
  "author": [
    {
      "family": "Vakkuri",
      "given": "Ville"
    },
    {
      "family": "Kemell",
      "given": "Kai-Kristian"
    },
    {
      "family": "Abrahamsson",
      "given": "Pekka"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2397"
  },
  "type": "chapter",
  "abstract": "Artificial intelligence enabled systems have been an inevitable part of everyday life. However, efficient software engineering principles and processes need to be considered and extended when developing AI- enabled systems. The objective of this study is to identify and classify software engineering challenges that are faced by different companies when developing software-intensive systems that incorporate machine learning components. Using case study approach, we explored the development of machine learning systems from six different companies across various domains and identified main software engineering challenges. The challenges are mapped into a proposed taxonomy that depicts the evolution of use of ML components in software-intensive system in industrial settings. Our study provides insights to software engineering community and research to guide discussions and future research into applied machine learning.",
  "ISBN": "978-3-030-19033-0",
  "note": "DOI: 10.1007/978-3-030-19034-7_14",
  "page": "227-243",
  "source": "ResearchGate",
  "title": "A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation",
  "title-short": "A Taxonomy of Software Engineering Challenges for Machine Learning Systems",
  "author": [
    {
      "family": "Lwakatare",
      "given": "Lucy Ellen"
    },
    {
      "family": "Raj",
      "given": "Aiswarya"
    },
    {
      "family": "Bosch",
      "given": "Jan"
    },
    {
      "family": "Olsson",
      "given": "Helena"
    },
    {
      "family": "Crnkovic",
      "given": "Ivica"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "AI Engineering",
    "Model Development",
    "Infrastructure",
    "Architecture Design"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2398"
  },
  "type": "paper-conference",
  "abstract": "Safety is one of the key aspects of road vehicles. With applications of machine learning and artificial intelligence (AI) technologies, driver assistance and automated driving systems have been rapidly developed. This paper identifies one of the emerging safety issues of automated driving systems: functional deficiencies resulting from limited sensing abilities and algorithmic performance. Safety validation problem and challenges for some methodologies provided by ISO 26262 are addressed. To this end, we provide a methodology framework for identifying functional deficiencies during system development. A novel methodology based on possibility theory and a fuzzy relation model, Causal Scenario Analysis (CSA), is introduced as one essential part in this framework. A traffic light handling case study is presented.",
  "container-title": "2018 IEEE Intelligent Vehicles Symposium (IV)",
  "DOI": "10.1109/IVS.2018.8500679",
  "event": "2018 IEEE Intelligent Vehicles Symposium (IV)",
  "note": "ISSN: 1931-0587",
  "page": "1918-1924",
  "source": "IEEE Xplore",
  "title": "Taming Functional Deficiencies of Automated Driving Systems: a Methodology Framework toward Safety Validation",
  "title-short": "Taming Functional Deficiencies of Automated Driving Systems",
  "author": [
    {
      "family": "Chen",
      "given": "M."
    },
    {
      "family": "Knapp",
      "given": "A."
    },
    {
      "family": "Pohl",
      "given": "M."
    },
    {
      "family": "Dietmayer",
      "given": "K."
    }
  ],
  "year": "2018",
  "DB": "Nascimento",
  "categorie": [
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b2399"
  },
  "type": "paper-conference",
  "abstract": "Intelligent systems are gaining in popularity and receiving increased media attention, but little is known about how people actually go about developing them. In this paper, we attempt to fill this gap through a set of field interviews that investigate how people develop intelligent systems that incorporate machine learning algorithms. The developers we interviewed were experienced at working with machine learning algorithms and dealing with the large amounts of data needed to develop intelligent systems. Despite their level of experience, we learned that they struggle to establish a repeatable process. They described problems with each step of the processes they perform, as well as cross-cutting issues that pervade multiple steps of their processes. The unique difficulties that developers like these face seem to point to a need for software engineering advances that address such machine learning systems, and we conclude by discussing this need and some of its implications.",
  "container-title": "2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  "DOI": "10.1109/VLHCC.2016.7739680",
  "event": "2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  "note": "ISSN: 1943-6106",
  "page": "162-170",
  "source": "IEEE Xplore",
  "title": "Trials and tribulations of developers of intelligent systems: A field study",
  "title-short": "Trials and tribulations of developers of intelligent systems",
  "author": [
    {
      "family": "Hill",
      "given": "C."
    },
    {
      "family": "Bellamy",
      "given": "R."
    },
    {
      "family": "Erickson",
      "given": "T."
    },
    {
      "family": "Burnett",
      "given": "M."
    }
  ],
  "year": "2016",
  "DB": "Nascimento",
  "categorie": [
    "Infrastructure",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239a"
  },
  "type": "paper-conference",
  "abstract": "Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.",
  "collection-title": "ASE '19",
  "container-title": "Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering",
  "DOI": "10.1109/ASE.2019.00080",
  "event-place": "San Diego, California",
  "ISBN": "978-1-72812-508-4",
  "page": "810–822",
  "publisher": "IEEE Press",
  "publisher-place": "San Diego, California",
  "source": "ACM Digital Library",
  "title": "An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms",
  "URL": "https://doi.org/10.1109/ASE.2019.00080",
  "author": [
    {
      "family": "Guo",
      "given": "Qianyu"
    },
    {
      "family": "Chen",
      "given": "Sen"
    },
    {
      "family": "Xie",
      "given": "Xiaofei"
    },
    {
      "family": "Ma",
      "given": "Lei"
    },
    {
      "family": "Hu",
      "given": "Qiang"
    },
    {
      "family": "Liu",
      "given": "Hongtao"
    },
    {
      "family": "Liu",
      "given": "Yang"
    },
    {
      "family": "Zhao",
      "given": "Jianjun"
    },
    {
      "family": "Li",
      "given": "Xiaohong"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Model Development",
    "Model Deployment",
    "AI Software Quality",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239b"
  },
  "type": "book",
  "abstract": "Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.",
  "ISBN": "978-1-4503-6668-7",
  "note": "DOI: 10.1145/3307630.3342419",
  "source": "ResearchGate",
  "title": "Reusability in Artificial Neural Networks: An Empirical Study",
  "title-short": "Reusability in Artificial Neural Networks",
  "author": [
    {
      "family": "Ghofrani",
      "given": "Javad"
    },
    {
      "family": "Kozegar",
      "given": "Ehsan"
    },
    {
      "family": "Bozorgmehr",
      "given": "Arezoo"
    },
    {
      "family": "Divband Soorati",
      "given": "Mohammad"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Model Development",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239c"
  },
  "type": "paper-conference",
  "abstract": "Machine Learning (ML) based solutions are becoming increasingly popular and pervasive. When testing such solutions, there is a tendency to focus on improving the ML metrics such as the F1-score and accuracy at the expense of ensuring business value and correctness by covering business requirements. In this work, we adapt test planning methods of classical software to ML solutions. We use combinatorial modeling methodology to define the space of business requirements and map it to the ML solution data, and use the notion of data slices to identify the weaker areas of the ML solution and strengthen them. We apply our approach to three real-world case studies and demonstrate its value.",
  "collection-title": "ESEC/FSE 2019",
  "container-title": "Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
  "DOI": "10.1145/3338906.3340442",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5572-8",
  "page": "1048–1058",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Bridging the gap between ML solutions and their business requirements using feature interactions",
  "URL": "https://doi.org/10.1145/3338906.3340442",
  "author": [
    {
      "family": "Barash",
      "given": "Guy"
    },
    {
      "family": "Farchi",
      "given": "Eitan"
    },
    {
      "family": "Jayaraman",
      "given": "Ilan"
    },
    {
      "family": "Raz",
      "given": "Orna"
    },
    {
      "family": "Tzoref-Brill",
      "given": "Rachel"
    },
    {
      "family": "Zalmanovici",
      "given": "Marcel"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239d"
  },
  "type": "paper-conference",
  "abstract": "Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",
  "collection-title": "ICSE '16",
  "container-title": "Proceedings of the 38th International Conference on Software Engineering",
  "DOI": "10.1145/2884781.2884783",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-3900-1",
  "page": "96–107",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "The emerging role of data scientists on software development teams",
  "URL": "https://doi.org/10.1145/2884781.2884783",
  "author": [
    {
      "family": "Kim",
      "given": "Miryung"
    },
    {
      "family": "Zimmermann",
      "given": "Thomas"
    },
    {
      "family": "DeLine",
      "given": "Robert"
    },
    {
      "family": "Begel",
      "given": "Andrew"
    }
  ],
  "year": "2016",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239e"
  },
  "type": "paper-conference",
  "DOI": "10.1109/ESEM.2019.8870187",
  "page": "1-11",
  "source": "ResearchGate",
  "title": "Why is Developing Machine Learning Applications Challenging? A Study on Stack Overflow Posts",
  "title-short": "Why is Developing Machine Learning Applications Challenging?",
  "author": [
    {
      "family": "Alshangiti",
      "given": "Moayad"
    },
    {
      "family": "Sapkota",
      "given": "Hitesh"
    },
    {
      "family": "Murukannaiah",
      "given": "Pradeep"
    },
    {
      "family": "Liu",
      "given": "Xumin"
    },
    {
      "family": "Yu",
      "given": "Qi"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b239f"
  },
  "type": "paper-conference",
  "abstract": "Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.",
  "container-title": "2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)",
  "DOI": "10.1109/IWESEP49350.2019.00017",
  "event": "2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)",
  "note": "ISSN: 2573-2021",
  "page": "49-495",
  "source": "IEEE Xplore",
  "title": "Studying Software Engineering Patterns for Designing Machine Learning Systems",
  "author": [
    {
      "family": "Washizaki",
      "given": "H."
    },
    {
      "family": "Uchida",
      "given": "H."
    },
    {
      "family": "Khomh",
      "given": "F."
    },
    {
      "family": "Guéhéneuc",
      "given": "Y."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a0"
  },
  "type": "paper-conference",
  "abstract": "Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.",
  "container-title": "2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "DOI": "10.1109/SEAA.2019.00030",
  "event": "2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
  "page": "140-147",
  "source": "IEEE Xplore",
  "title": "Data Management Challenges for Deep Learning",
  "author": [
    {
      "family": "Munappy",
      "given": "A."
    },
    {
      "family": "Bosch",
      "given": "J."
    },
    {
      "family": "Olsson",
      "given": "H. H."
    },
    {
      "family": "Arpteg",
      "given": "A."
    },
    {
      "family": "Brinne",
      "given": "B."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality",
    "Data Management"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a1"
  },
  "type": "paper-conference",
  "abstract": "Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.",
  "DOI": "10.1109/SEAA.2018.00018",
  "page": "50-59",
  "source": "ResearchGate",
  "title": "Software Engineering Challenges of Deep Learning",
  "author": [
    {
      "family": "Arpteg",
      "given": "Anders"
    },
    {
      "family": "Brinne",
      "given": "Bjorn"
    },
    {
      "family": "Crnkovic-Friis",
      "given": "Luka"
    },
    {
      "family": "Bosch",
      "given": "Jan"
    }
  ],
  "year": "2018",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality",
    "Project Management",
    "Model Deployment",
    "Infrastructure",
    "Testing",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a2"
  },
  "type": "paper-conference",
  "abstract": "Recently, machine learning systems with inference engines have been widely used for a variety of purposes (such as prediction and classification) in our society. While it is quite important to keep the services provided by these machine learning systems stable, maintaining stability can be difficult given the nature of machine learning systems whose behaviors can be determined by program codes and input data. Therefore, quick troubleshooting (problem localization, rollback, etc.) is necessary. However, common machine learning systems with three-layer architectural patterns complicate the troubleshooting process because of their tightly coupled functions (e.g., business logic coded from design and inference engine derived from data). To solve the problem, we propose a novel architectural pattern for machine learning systems in which components for business logic and components for machine learning are separated. This architectural pattern helps operators break down the failures into a business logic part and a ML-specific part, and they can rollback the inference engine independent of the business logic when the inference engine has some problems. Through a practical case study scenario, we will show how our architectural pattern can make troubleshooting easier than common three-layer architecture.",
  "container-title": "2019 IEEE International Conference on Software Architecture Companion (ICSA-C)",
  "DOI": "10.1109/ICSA-C.2019.00055",
  "event": "2019 IEEE International Conference on Software Architecture Companion (ICSA-C)",
  "page": "267-274",
  "source": "IEEE Xplore",
  "title": "Machine Learning System Architectural Pattern for Improving Operational Stability",
  "author": [
    {
      "family": "Yokoyama",
      "given": "H."
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a3"
  },
  "type": "article-journal",
  "abstract": "The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",
  "container-title": "IEEE Transactions on Software Engineering",
  "DOI": "10.1109/TSE.2017.2754374",
  "ISSN": "1939-3520",
  "issue": "11",
  "note": "event: IEEE Transactions on Software Engineering",
  "page": "1024-1038",
  "source": "IEEE Xplore",
  "title": "Data Scientists in Software Teams: State of the Art and Challenges",
  "title-short": "Data Scientists in Software Teams",
  "volume": "44",
  "author": [
    {
      "family": "Kim",
      "given": "M."
    },
    {
      "family": "Zimmermann",
      "given": "T."
    },
    {
      "family": "DeLine",
      "given": "R."
    },
    {
      "family": "Begel",
      "given": "A."
    }
  ],
  "year": "2017",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Infrastructure",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a4"
  },
  "type": "paper-conference",
  "abstract": "Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.",
  "container-title": "2016 Information Theory and Applications Workshop (ITA)",
  "DOI": "10.1109/ITA.2016.7888195",
  "event": "2016 Information Theory and Applications Workshop (ITA)",
  "page": "1-5",
  "source": "IEEE Xplore",
  "title": "Engineering safety in machine learning",
  "author": [
    {
      "family": "Varshney",
      "given": "K. R."
    }
  ],
  "year": "2016",
  "DB": "Nascimento",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Model Development",
    "Infrastructure",
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a5"
  },
  "type": "book",
  "abstract": "SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections. We identify and discuss the challenges that we faced during this collaborative research and development project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges. We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.",
  "source": "ResearchGate",
  "title": "Machine Learning Software Engineering in Practice: An Industrial Case Study",
  "title-short": "Machine Learning Software Engineering in Practice",
  "author": [
    {
      "family": "Rahman",
      "given": "Md Saidur"
    },
    {
      "family": "Rivera",
      "given": "Emilio"
    },
    {
      "family": "Khomh",
      "given": "Foutse"
    },
    {
      "family": "Guéhéneuc",
      "given": "Yann-Gaël"
    },
    {
      "family": "Lehnert",
      "given": "Bernd"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Testing",
    "Requirement Engineering",
    "Architecture Design"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a6"
  },
  "type": "webpage",
  "container-title": "Google Developers",
  "language": "en",
  "title": "Rules of Machine Learning: | ML Universal Guides",
  "title-short": "Rules of Machine Learning",
  "URL": "https://developers.google.com/machine-learning/guides/rules-of-ml?hl=de",
  "accessed": {
    "date-parts": [
      [
        "2021",
        4,
        4
      ]
    ]
  },
  "year": "2016",
  "DB": "Nascimento",
  "categorie": [
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a7"
  },
  "type": "chapter",
  "abstract": "Despite the hype around machine learning (ML), many organizations are struggling to derive business value from ML capabilities. Design patterns have long been used in software engineering to enhance design effectiveness and to speed up the development process. The contribution of this paper is two-fold. First, it introduces solution patterns as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. Second, it reports on the feasibility, expressiveness, and usefulness of solution patterns for ML, in collaboration with an industry partner. It provides a prototype architecture for supporting the use of solution patterns in real world scenarios. It presents a proof-of-concept implementation of the architecture and illustrates its feasibility. Findings from the collaboration suggest that solution patterns can have a positive impact on ML design and development efforts.",
  "ISBN": "978-3-030-21289-6",
  "note": "DOI: 10.1007/978-3-030-21290-2_39",
  "page": "627-642",
  "source": "ResearchGate",
  "title": "Solution Patterns for Machine Learning",
  "author": [
    {
      "family": "Nalchigar",
      "given": "Soroosh"
    },
    {
      "family": "Yu",
      "given": "Eric"
    },
    {
      "family": "Obeidi",
      "given": "Yazan"
    },
    {
      "family": "Carbajales",
      "given": "Sebastian"
    },
    {
      "family": "Green",
      "given": "John"
    },
    {
      "family": "Chan",
      "given": "Allen"
    }
  ],
  "year": "2019",
  "DB": "Nascimento",
  "categorie": [
    "Requirement Engineering"
  ]
},{
  "_id": {
    "$oid": "60db1bc3b3da9102496b23a8"
  },
  "type": "article-journal",
  "abstract": "Machine learning offers a fantastically powerful toolkit for building useful com-plex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",
  "container-title": "NIPS",
  "journalAbbreviation": "NIPS",
  "page": "2494-2502",
  "source": "ResearchGate",
  "title": "Hidden Technical Debt in Machine Learning Systems",
  "author": [
    {
      "family": "Sculley",
      "given": "D"
    },
    {
      "family": "Holt",
      "given": "Gary"
    },
    {
      "family": "Golovin",
      "given": "Daniel"
    },
    {
      "family": "Davydov",
      "given": "Eugene"
    },
    {
      "family": "Phillips",
      "given": "Todd"
    },
    {
      "family": "Ebner",
      "given": "Dietmar"
    },
    {
      "family": "Chaudhary",
      "given": "Vinay"
    },
    {
      "family": "Young",
      "given": "Michael"
    },
    {
      "family": "Dennison",
      "given": "Dan"
    }
  ],
  "year": "2015",
  "DB": "Nascimento",
  "categorie": [
    "Data Management",
    "Project Management",
    "Infrastructure",
    "Testing",
    "Architecture Design"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23a9"
  },
  "type": "paper-conference",
  "abstract": "Recommender systems are a key technology for many online services including e-commerce, movies, music, and news. Online retailers use product recommender systems to help users discover items that they may like. However, building a large-scale product recommender system is a challenging task. The problems of sparsity and cold-start are much more pronounced in this domain. Large online retailers have used good recommendations to drive user engagement and improve revenue, but the complexity involved is a roadblock to widespread adoption by smaller retailers. In this paper, we tackle the problem of generating product recommendations for tens of thousands of online retailers. Sigmund is an industrial-scale system for providing recommendations as a service. Sigmund was deployed to production in early 2014 and has been serving retailers every day. We describe the design choices that we made in order to train accurate matrix factorization models at minimal cost. We also share the lessons we learned from this experience - both from a machine learning perspective and a systems perspective. We hope that these lessons are useful for building future machine-learning services.",
  "container-title": "2018 IEEE 34th International Conference on Data Engineering (ICDE)",
  "DOI": "10.1109/ICDE.2018.00159",
  "event": "2018 IEEE 34th International Conference on Data Engineering (ICDE)",
  "note": "ISSN: 2375-026X",
  "page": "1404-1413",
  "source": "IEEE Xplore",
  "title": "Recommendations for All: Solving Thousands of Recommendation Problems Daily",
  "title-short": "Recommendations for All",
  "author": [
    {
      "family": "Kanagal",
      "given": "B."
    },
    {
      "family": "Tata",
      "given": "S."
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Infrastructure",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23aa"
  },
  "type": "paper-conference",
  "abstract": "Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a well-known graph embedding framework. We first construct an item graph from users' behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao's live production environment.",
  "collection-title": "KDD '18",
  "container-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
  "DOI": "10.1145/3219819.3219869",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5552-0",
  "page": "839–848",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba",
  "URL": "https://doi.org/10.1145/3219819.3219869",
  "author": [
    {
      "family": "Wang",
      "given": "Jizhe"
    },
    {
      "family": "Huang",
      "given": "Pipei"
    },
    {
      "family": "Zhao",
      "given": "Huan"
    },
    {
      "family": "Zhang",
      "given": "Zhibo"
    },
    {
      "family": "Zhao",
      "given": "Binqiang"
    },
    {
      "family": "Lee",
      "given": "Dik Lun"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ab"
  },
  "type": "book",
  "abstract": "Facebook Marketplace is quickly gaining momentum among consumers as a favored customer-to-customer (C2C) product trading platform. The recommendation system behind it helps to significantly improve the user experience. Building the recommendation system for Facebook Marketplace is challenging for two reasons: 1) Scalability: the number of products in Facebook Marketplace is huge. Tens of thousands of products need to be scored and recommended within a couple hundred milliseconds for millions of users every day; 2) Cold start: the life span of the C2C products is very short and the user activities on the products are sparse. Thus it is difficult to accumulate enough product level signals for recommendation and we are facing a significant cold start issue. In this paper, we propose to address both the scalability and the cold-start issue by building a collaborative multi-modal deep learning based retrieval system where the compact embeddings for the users and the products are trained with the multi-modal content information. This system shows significant improvement over the benchmark in online and off-line experiments: In the online experiment, it increases the number of messages initiated by the buyer to the seller by +26.95%; in the off-line experiment, it improves the prediction accuracy by +9.58%.",
  "source": "ResearchGate",
  "title": "Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace",
  "author": [
    {
      "family": "Zheng",
      "given": "Lu"
    },
    {
      "family": "Tan",
      "given": "Zhao"
    },
    {
      "family": "Han",
      "given": "Kun"
    },
    {
      "family": "Mao",
      "given": "Ren"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ac"
  },
  "type": "article-journal",
  "abstract": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.",
  "container-title": "arXiv:1606.07792 [cs, stat]",
  "note": "arXiv: 1606.07792",
  "source": "arXiv.org",
  "title": "Wide & Deep Learning for Recommender Systems",
  "URL": "http://arxiv.org/abs/1606.07792",
  "author": [
    {
      "family": "Cheng",
      "given": "Heng-Tze"
    },
    {
      "family": "Koc",
      "given": "Levent"
    },
    {
      "family": "Harmsen",
      "given": "Jeremiah"
    },
    {
      "family": "Shaked",
      "given": "Tal"
    },
    {
      "family": "Chandra",
      "given": "Tushar"
    },
    {
      "family": "Aradhye",
      "given": "Hrishi"
    },
    {
      "family": "Anderson",
      "given": "Glen"
    },
    {
      "family": "Corrado",
      "given": "Greg"
    },
    {
      "family": "Chai",
      "given": "Wei"
    },
    {
      "family": "Ispir",
      "given": "Mustafa"
    },
    {
      "family": "Anil",
      "given": "Rohan"
    },
    {
      "family": "Haque",
      "given": "Zakaria"
    },
    {
      "family": "Hong",
      "given": "Lichan"
    },
    {
      "family": "Jain",
      "given": "Vihan"
    },
    {
      "family": "Liu",
      "given": "Xiaobing"
    },
    {
      "family": "Shah",
      "given": "Hemal"
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Infrastructure",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ad"
  },
  "type": "article-journal",
  "container-title": "Information Systems",
  "DOI": "10.1016/j.is.2017.08.006",
  "journalAbbreviation": "Information Systems",
  "page": "152-163",
  "source": "ResearchGate",
  "title": "Mobile recommendations based on interest prediction from consumer's installed apps – Insights from a large-scale field study",
  "volume": "71",
  "author": [
    {
      "family": "Frey",
      "given": "Remo"
    },
    {
      "family": "Xu",
      "given": "Runhua"
    },
    {
      "family": "Ammendola",
      "given": "Christian"
    },
    {
      "family": "Moling",
      "given": "Omar"
    },
    {
      "family": "Giglio",
      "given": "Giuseppe"
    },
    {
      "family": "Ilic",
      "given": "Alexander"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Testing",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ae"
  },
  "type": "paper-conference",
  "abstract": "YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.",
  "collection-title": "RecSys '16",
  "container-title": "Proceedings of the 10th ACM Conference on Recommender Systems",
  "DOI": "10.1145/2959100.2959190",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4035-9",
  "page": "191–198",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Deep Neural Networks for YouTube Recommendations",
  "URL": "https://doi.org/10.1145/2959100.2959190",
  "author": [
    {
      "family": "Covington",
      "given": "Paul"
    },
    {
      "family": "Adams",
      "given": "Jay"
    },
    {
      "family": "Sargin",
      "given": "Emre"
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Testing",
    "Model Development",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23af"
  },
  "type": "article-journal",
  "abstract": "Related Pins is the Web-scale recommender system that powers over 40% of user engagement on Pinterest. This paper is a longitudinal study of three years of its development, exploring the evolution of the system and its components from prototypes to present state. Each component was originally built with many constraints on engineering effort and computational resources, so we prioritized the simplest and highest-leverage solutions. We show how organic growth led to a complex system and how we managed this complexity. Many challenges arose while building this system, such as avoiding feedback loops, evaluating performance, activating content, and eliminating legacy heuristics. Finally, we offer suggestions for tackling these challenges when engineering Web-scale recommender systems.",
  "source": "ResearchGate",
  "title": "Related Pins at Pinterest: The Evolution of a Real-World Recommender System",
  "title-short": "Related Pins at Pinterest",
  "author": [
    {
      "family": "Liu",
      "given": "David"
    },
    {
      "family": "Rogers",
      "given": "Stephanie"
    },
    {
      "family": "Shiau",
      "given": "Raymond"
    },
    {
      "family": "Kislyuk",
      "given": "Dmitry"
    },
    {
      "family": "Ma",
      "given": "Kevin"
    },
    {
      "family": "Zhong",
      "given": "Zhigang"
    },
    {
      "family": "Liu",
      "given": "Jenny"
    },
    {
      "family": "Jing",
      "given": "Yushi"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Model Deployment",
    "Infrastructure",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b0"
  },
  "type": "article-journal",
  "abstract": "User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80% of all user engagement on Pinterest.",
  "source": "ResearchGate",
  "title": "Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time",
  "title-short": "Pixie",
  "author": [
    {
      "family": "Eksombatchai",
      "given": "Chantat"
    },
    {
      "family": "Jindal",
      "given": "Pranav"
    },
    {
      "family": "Liu",
      "given": "Jerry"
    },
    {
      "family": "Liu",
      "given": "Yuchen"
    },
    {
      "family": "Sharma",
      "given": "Rahul"
    },
    {
      "family": "Sugnet",
      "given": "Charles"
    },
    {
      "family": "Ulrich",
      "given": "Mark"
    },
    {
      "family": "Leskovec",
      "given": "Jure"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b1"
  },
  "type": "article-journal",
  "abstract": "This paper presents GraphJet, a new graph-based system for generating content recommendations at Twitter. As motivation, we trace the evolution of our formulation and approach to the graph recommendation problem, embodied in successive generations of systems. Two trends can be identified: supplementing batch with real-time processing and a broadening of the scope of recommendations from users to content. Both of these trends come together in Graph-Jet, an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements a simple API, but one that is sufficiently expressive to support a range of recommendation algorithms based on random walks that we have refined over the years. Similar to Cassovary, a previous graph recommendation engine developed at Twitter, GraphJet assumes that the entire graph can be held in memory on a single server. The system organizes the interaction graph into temporally-partitioned index segments that hold adjacency lists. GraphJet is able to support rapid ingestion of edges while concurrently serving lookup queries through a combination of compact edge encoding and a dynamic memory allocation scheme that exploits power-law characteristics of the graph. Each GraphJet server ingests up to one million graph edges per second, and in steady state, computes up to 500 recommendations per second, which translates into several million edge read operations per second.",
  "container-title": "Proceedings of the VLDB Endowment",
  "DOI": "10.14778/3007263.3007267",
  "ISSN": "2150-8097",
  "issue": "13",
  "journalAbbreviation": "Proc. VLDB Endow.",
  "page": "1281–1292",
  "source": "September 2016",
  "title": "GraphJet: real-time content recommendations at twitter",
  "title-short": "GraphJet",
  "URL": "https://doi.org/10.14778/3007263.3007267",
  "volume": "9",
  "author": [
    {
      "family": "Sharma",
      "given": "Aneesh"
    },
    {
      "family": "Jiang",
      "given": "Jerry"
    },
    {
      "family": "Bommannavar",
      "given": "Praveen"
    },
    {
      "family": "Larson",
      "given": "Brian"
    },
    {
      "family": "Lin",
      "given": "Jimmy"
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b2"
  },
  "type": "paper-conference",
  "abstract": "Machine learning techniques have proved effective in recommender systems and other applications, yet teams working to deploy them lack many of the advantages that those in more established software disciplines today take for granted. The well-known Agile methodology advances projects in a chain of rapid development cycles, with subsequent steps often informed by production experiments. Support for such workflow in machine learning applications remains primitive. The platform developed at if(we) embodies a specific machine learning approach and a rigorous data architecture constraint, so allowing teams to work in rapid iterative cycles. We require models to consume data from a time-ordered event history, and we focus on facilitating creative feature engineering. We make it practical for data scientists to use the same model code in development and in production deployment, and make it practical for them to collaborate on complex models. We deliver real-time recommendations at scale, returning top results from among 10,000,000 candidates with sub-second response times and incorporating new updates in just a few seconds. Using the approach and architecture described here, our team can routinely go from ideas for new models to production-validated results within two weeks.",
  "collection-title": "KDD '15",
  "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/2783258.2788628",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-3664-2",
  "page": "2059–2068",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "An Architecture for Agile Machine Learning in Real-Time Applications",
  "URL": "https://doi.org/10.1145/2783258.2788628",
  "author": [
    {
      "family": "Schleier-Smith",
      "given": "Johann"
    }
  ],
  "year": "2015",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Testing",
    "Model Deployment"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b3"
  },
  "type": "paper-conference",
  "abstract": "Online social networking services distribute various types of messages to their members. Common types of messages include news, connection requests, membership notifications, promotions and event notifications. Such communication, if used judiciously, can provide an enormous value to members thereby keeping them engaged. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member's mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member's interests. It may also result in a poor brand perception of the networking service. In this paper we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system and experimental results from online A/B tests.",
  "collection-title": "KDD '16",
  "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/2939672.2939692",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4232-2",
  "page": "97–106",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Email Volume Optimization at LinkedIn",
  "URL": "https://doi.org/10.1145/2939672.2939692",
  "author": [
    {
      "family": "Gupta",
      "given": "Rupesh"
    },
    {
      "family": "Liang",
      "given": "Guanfeng"
    },
    {
      "family": "Tseng",
      "given": "Hsiao-Ping"
    },
    {
      "family": "Holur Vijay",
      "given": "Ravi Kiran"
    },
    {
      "family": "Chen",
      "given": "Xiaoyu"
    },
    {
      "family": "Rosales",
      "given": "Romer"
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Testing",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b4"
  },
  "type": "paper-conference",
  "abstract": "Notifications (including emails, mobile / desktop push notifications, SMS, etc.) are very effective channels for online services to engage with users and drive user engagement metrics and other business metrics. One of the most important and challenging problems in a production notification system is to decide the right frequency for each user. In this paper, we propose a novel machine learning approach to decide notification volume for each user such that long term user engagement is optimized. We will also discuss a few practical issues and design choices we have made. The new system has been deployed to production at Pinterest in mid 2017 and significantly reduced notification volume and improved CTR of notifications and site engagement metrics compared with the previous machine learning approach.",
  "collection-title": "KDD '18",
  "container-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
  "DOI": "10.1145/3219819.3219906",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5552-0",
  "page": "1012–1020",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Notification Volume Control and Optimization System at Pinterest",
  "URL": "https://doi.org/10.1145/3219819.3219906",
  "author": [
    {
      "family": "Zhao",
      "given": "Bo"
    },
    {
      "family": "Narita",
      "given": "Koichiro"
    },
    {
      "family": "Orten",
      "given": "Burkay"
    },
    {
      "family": "Egan",
      "given": "John"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Testing",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b5"
  },
  "type": "paper-conference",
  "abstract": "In this paper we focus on the problem of optimizing email volume for maximizing sitewide engagement of an online social networking service. Email volume optimization approaches published in the past have proposed optimization of email volume for maximization of engagement metrics which are impacted exclusively by email; for example, the number of sessions that begin with clicks on links within emails. The impact of email on such downstream engagement metrics can be estimated easily because of the ease of attribution of such an engagement event to an email. However, this framework is limited in its view of the ecosystem of the networking service which comprises of several tools and utilities that contribute towards delivering value to members; with email being just one such utility. Thus, in this paper we depart from previous approaches by exploring and optimizing the contribution of email to this ecosystem. In particular, we present and contrast the differential impact of email on sitewide engagement metrics for various types of users. We propose a new email volume optimization approach which maximizes sitewide engagement metrics, such as the total number of active users. This is in sharp contrast to the previous approaches whose objective has been maximization of downstream engagement metrics. We present details of our prediction function for predicting the impact of emails on a user's activeness on the mobile or web application. We describe how certain approximations to this prediction function can be made for solving the volume optimization problem, and present results from online A/B tests.",
  "collection-title": "CIKM '17",
  "container-title": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management",
  "DOI": "10.1145/3132847.3132849",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4918-5",
  "page": "1947–1955",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Optimizing Email Volume For Sitewide Engagement",
  "URL": "https://doi.org/10.1145/3132847.3132849",
  "author": [
    {
      "family": "Gupta",
      "given": "Rupesh"
    },
    {
      "family": "Liang",
      "given": "Guanfeng"
    },
    {
      "family": "Rosales",
      "given": "Romer"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b6"
  },
  "type": "paper-conference",
  "abstract": "In the 'Big Data' era, many real-world applications like search involve the ranking problem for a large number of items. It is important to obtain effective ranking results and at the same time obtain the results efficiently in a timely manner for providing good user experience and saving computational costs. Valuable prior research has been conducted for learning to efficiently rank like the cascade ranking (learning) model, which uses a sequence of ranking functions to progressively filter some items and rank the remaining items. However, most existing research of learning to efficiently rank in search is studied in a relatively small computing environments with simulated user queries. This paper presents novel research and thorough study of designing and deploying a Cascade model in a Large-scale Operational E-commerce Search application (CLOES), which deals with hundreds of millions of user queries per day with hundreds of servers. The challenge of the real-world application provides new insights for research: 1). Real-world search applications often involve multiple factors of preferences or constraints with respect to user experience and computational costs such as search accuracy, search latency, size of search results and total CPU cost, while most existing search solutions only address one or two factors; 2). Effectiveness of e-commerce search involves multiple types of user behaviors such as click and purchase, while most existing cascade ranking in search only models the click behavior. Based on these observations, a novel cascade ranking model is designed and deployed in an operational e-commerce search application. An extensive set of experiments demonstrate the advantage of the proposed work to address multiple factors of effectiveness, efficiency and user experience in the real-world application.",
  "collection-title": "KDD '17",
  "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/3097983.3098011",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4887-4",
  "page": "1557–1565",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Cascade Ranking for Operational E-commerce Search",
  "URL": "https://doi.org/10.1145/3097983.3098011",
  "author": [
    {
      "family": "Liu",
      "given": "Shichen"
    },
    {
      "family": "Xiao",
      "given": "Fei"
    },
    {
      "family": "Ou",
      "given": "Wenwu"
    },
    {
      "family": "Si",
      "given": "Luo"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b7"
  },
  "type": "article-journal",
  "abstract": "Tasks such as search and recommendation have become increas- ingly important for E-commerce to deal with the information over- load problem. To meet the diverse needs of di erent users, person- alization plays an important role. In many large portals such as Taobao and Amazon, there are a bunch of di erent types of search and recommendation tasks operating simultaneously for person- alization. However, most of current techniques address each task separately. This is suboptimal as no information about users shared across di erent tasks. In this work, we propose to learn universal user representations across multiple tasks for more e ective personalization. In partic- ular, user behavior sequences (e.g., click, bookmark or purchase of products) are modeled by LSTM and attention mechanism by integrating all the corresponding content, behavior and temporal information. User representations are shared and learned in an end-to-end setting across multiple tasks. Bene ting from better information utilization of multiple tasks, the user representations are more e ective to re ect their interests and are more general to be transferred to new tasks. We refer this work as Deep User Perception Network (DUPN) and conduct an extensive set of o ine and online experiments. Across all tested ve di erent tasks, our DUPN consistently achieves better results by giving more e ective user representations. Moreover, we deploy DUPN in large scale operational tasks in Taobao. Detailed implementations, e.g., incre- mental model updating, are also provided to address the practical issues for the real world applications.",
  "container-title": "arXiv:1805.10727 [cs, stat]",
  "note": "arXiv: 1805.10727",
  "source": "arXiv.org",
  "title": "Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks",
  "title-short": "Perceive Your Users in Depth",
  "URL": "http://arxiv.org/abs/1805.10727",
  "author": [
    {
      "family": "Ni",
      "given": "Yabo"
    },
    {
      "family": "Ou",
      "given": "Dan"
    },
    {
      "family": "Liu",
      "given": "Shichen"
    },
    {
      "family": "Li",
      "given": "Xiang"
    },
    {
      "family": "Ou",
      "given": "Wenwu"
    },
    {
      "family": "Zeng",
      "given": "Anxiang"
    },
    {
      "family": "Si",
      "given": "Luo"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b8"
  },
  "type": "paper-conference",
  "abstract": "LinkedIn has grown to become a platform hosting diverse sources of information ranging from member profiles, jobs, professional groups, slideshows etc. Given the existence of multiple sources, when a member issues a query like \"software engineer\", the member could look for software engineer profiles, jobs or professional groups. To tackle this problem, we exploit a data-driven approach that extracts searcher intents from their profile data and recent activities at a large scale. The intents such as job seeking, hiring, content consuming are used to construct features to personalize federated search experience. We tested the approach on the LinkedIn homepage and A/B tests show significant improvements in member engagement. As of writing this paper, the approach powers all of federated search on LinkedIn homepage.",
  "collection-title": "CIKM '15",
  "container-title": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management",
  "DOI": "10.1145/2806416.2806615",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-3794-6",
  "page": "1699–1702",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Personalized Federated Search at LinkedIn",
  "URL": "https://doi.org/10.1145/2806416.2806615",
  "author": [
    {
      "family": "Arya",
      "given": "Dhruv"
    },
    {
      "family": "Ha-Thuc",
      "given": "Viet"
    },
    {
      "family": "Sinha",
      "given": "Shakti"
    }
  ],
  "year": "2015",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Data Management",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23b9"
  },
  "type": "paper-conference",
  "abstract": "Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.",
  "container-title": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)",
  "DOI": "10.1109/HPCA.2018.00059",
  "event": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)",
  "note": "ISSN: 2378-203X",
  "page": "620-629",
  "source": "IEEE Xplore",
  "title": "Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective",
  "title-short": "Applied Machine Learning at Facebook",
  "author": [
    {
      "family": "Hazelwood",
      "given": "K."
    },
    {
      "family": "Bird",
      "given": "S."
    },
    {
      "family": "Brooks",
      "given": "D."
    },
    {
      "family": "Chintala",
      "given": "S."
    },
    {
      "family": "Diril",
      "given": "U."
    },
    {
      "family": "Dzhulgakov",
      "given": "D."
    },
    {
      "family": "Fawzy",
      "given": "M."
    },
    {
      "family": "Jia",
      "given": "B."
    },
    {
      "family": "Jia",
      "given": "Y."
    },
    {
      "family": "Kalro",
      "given": "A."
    },
    {
      "family": "Law",
      "given": "J."
    },
    {
      "family": "Lee",
      "given": "K."
    },
    {
      "family": "Lu",
      "given": "J."
    },
    {
      "family": "Noordhuis",
      "given": "P."
    },
    {
      "family": "Smelyanskiy",
      "given": "M."
    },
    {
      "family": "Xiong",
      "given": "L."
    },
    {
      "family": "Wang",
      "given": "X."
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ba"
  },
  "type": "paper-conference",
  "abstract": "We present the problem of click-through prediction for advertising in Twitter timeline, which displays a stream of Tweets from accounts a user choose to follow. Traditional computational advertising usually appears in two forms: sponsored search that places ads onto the search result page when a query is issued to a search engine, and contextual advertising that places ads onto a regular, usually static Web page. Compared with these two paradigms, placing ads into a Tweet stream is particularly challenging given the nature of the data stream: the context into which an ad can be placed updates dynamically and never replicates. Every ad is therefore placed into a unique context. This makes the information available for training a machine learning model extremely sparse. In this study, we propose a learning-to-rank method which not only addresses the sparsity of training signals but also can be trained and updated online. The proposed method is evaluated using both offline experiments and online A/B tests, which involve very large collections of Twitter data and real Twitter users. Results of the experiments prove the effectiveness and efficiency of our solution, and its superiority over the current production model adopted by Twitter.",
  "collection-title": "KDD '15",
  "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/2783258.2788582",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-3664-2",
  "page": "1959–1968",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Click-through Prediction for Advertising in Twitter Timeline",
  "URL": "https://doi.org/10.1145/2783258.2788582",
  "author": [
    {
      "family": "Li",
      "given": "Cheng"
    },
    {
      "family": "Lu",
      "given": "Yue"
    },
    {
      "family": "Mei",
      "given": "Qiaozhu"
    },
    {
      "family": "Wang",
      "given": "Dong"
    },
    {
      "family": "Pandey",
      "given": "Sandeep"
    }
  ],
  "year": "2015",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Data Management",
    "AI Software Quality",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23bb"
  },
  "type": "paper-conference",
  "abstract": "The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the 'large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10.",
  "DOI": "10.1145/2939672.2939790",
  "page": "1125-1134",
  "source": "ResearchGate",
  "title": "Robust Large-Scale Machine Learning in the Cloud",
  "author": [
    {
      "family": "Rendle",
      "given": "Steffen"
    },
    {
      "family": "Fetterly",
      "given": "Dennis"
    },
    {
      "family": "Shekita",
      "given": "Eugene"
    },
    {
      "family": "Su",
      "given": "Bor-yiing"
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23bc"
  },
  "type": "article-journal",
  "abstract": "For a company looking to provide delightful user experiences, it is of paramount importance to take care of any customer issues. This paper proposes COTA, a system to improve speed and reliability of customer support for end users through automated ticket classification and answers selection for support representatives. Two machine learning and natural language processing techniques are demonstrated: one relying on feature engineering (COTA v1) and the other exploiting raw signals through deep learning architectures (COTA v2). COTA v1 employs a new approach that converts the multi-classification task into a ranking problem, demonstrating significantly better performance in the case of thousands of classes. For COTA v2, we propose an Encoder-Combiner-Decoder, a novel deep learning architecture that allows for heterogeneous input and output feature types and injection of prior knowledge through network architecture choices. This paper compares these models and their variants on the task of ticket classification and answer selection, showing model COTA v2 outperforms COTA v1, and analyzes their inner workings and shortcomings. Finally, an A/B test is conducted in a production setting validating the real-world impact of COTA in reducing issue resolution time by 10 percent without reducing customer satisfaction.",
  "container-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
  "DOI": "10.1145/3219819.3219851",
  "note": "arXiv: 1807.01337",
  "page": "586-595",
  "source": "arXiv.org",
  "title": "COTA: Improving the Speed and Accuracy of Customer Support through Ranking and Deep Networks",
  "title-short": "COTA",
  "URL": "http://arxiv.org/abs/1807.01337",
  "author": [
    {
      "family": "Molino",
      "given": "Piero"
    },
    {
      "family": "Zheng",
      "given": "Huaixiu"
    },
    {
      "family": "Wang",
      "given": "Yi-Chia"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23bd"
  },
  "type": "chapter",
  "ISBN": "978-3-319-71272-7",
  "note": "DOI: 10.1007/978-3-319-71273-4_20",
  "page": "241-252",
  "source": "ResearchGate",
  "title": "Session-Based Fraud Detection in Online E-Commerce Transactions Using Recurrent Neural Networks",
  "author": [
    {
      "family": "Wang",
      "given": "Shuhao"
    },
    {
      "family": "Liu",
      "given": "Cancheng"
    },
    {
      "family": "Gao",
      "given": "Xiang"
    },
    {
      "family": "Qu",
      "given": "Hongtao"
    },
    {
      "family": "Xu",
      "given": "Wei"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23be"
  },
  "type": "article-journal",
  "abstract": "The rapid growth of mobile Internet technologies has induced a dramatic increase in mobile payments as well as concomitant mobile transaction fraud. As the first step of mobile transactions, bankcard enrollment on mobile devices has become the primary target of fraud attempts. Although no immediate financial loss is incurred after a fraud attempt, subsequent fraudulent transactions can be quickly executed and could easily deceive the fraud detection systems if the fraud attempt succeeds at the bankcard enrollment step. In recent years, financial institutions and service providers have implemented rule-based expert systems and adopted short message service (SMS) user authentication to address this problem. However, the above solution is inadequate to face the challenges of data loss and social engineering. In this study, we introduce several traditional machine learning algorithms and finally choose the improved gradient boosting decision tree (GBDT) algorithm software library for use in a real system, namely, XGBoost. We further expand multiple features based on analysis of the enrollment behavior and plan to add historical transactions in future studies. Subsequently, we use a real card enrollment dataset covering the year 2017, provided by a worldwide payment processor. The results and framework are adopted and absorbed into a new design for a mobile payment fraud detection system within the Chinese payment processor.",
  "container-title": "Frontiers of Information Technology & Electronic Engineering",
  "DOI": "10.1631/FITEE.1800580",
  "journalAbbreviation": "Frontiers of Information Technology & Electronic Engineering",
  "page": "1537-1545",
  "source": "ResearchGate",
  "title": "Fraud detection within bankcard enrollment on mobile device based payment using machine learning",
  "volume": "19",
  "author": [
    {
      "family": "Zhou",
      "given": "Hao"
    },
    {
      "family": "Chai",
      "given": "Hong-feng"
    },
    {
      "family": "Qiu",
      "given": "Mao-lin"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23bf"
  },
  "type": "paper-conference",
  "abstract": "Traffic sign recognition (TSR) systems on the vehicles can collect posted speed limit sign information and have been in commercial usage since 2008. A daily-updated auto-pilot map can be constructed based on the massive amounts of TSR observations from multiple consumer vehicles; the data is then aggregated, filtered and processed, and the learned posted speed limit signs can be finally transferred to vehicles with high-coverage and real-time speed limit information. Compared with the direct sign detection by TSR systems, the auto-pilot map can complement the current detection errors, reduce the camera cost and provide a continuous speed limit information for autonomous vehicle applications. A pipeline of methods are specifically designed to deliver our research purpose by making full utilization of TSR observations and HERE map. Experimental results indicate that our proposed algorithms and methods can construct an auto-pilot map with an overall accuracy of 95.8%. It is also expected to update the speed limit information in a map at a faster pace than the traditional map since we are using sensors of customer vehicles instead of dedicated map construction vehicles. The utility of our proposed auto-pilot map opens a new perspective in autonomous driving.",
  "DOI": "10.1145/3274895.3274951",
  "page": "468-471",
  "source": "ResearchGate",
  "title": "Insert beyond the traffic sign recognition: constructing an auto-pilot map for autonomous vehicles",
  "title-short": "Insert beyond the traffic sign recognition",
  "author": [
    {
      "family": "Zhang",
      "given": "Zhenhua"
    },
    {
      "family": "Stenneth",
      "given": "Leon"
    },
    {
      "family": "Marappan",
      "given": "Ram"
    },
    {
      "family": "Sebastian",
      "given": "Zaba"
    },
    {
      "family": "Yu",
      "given": "Philip"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development",
    "Testing",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c0"
  },
  "type": "paper-conference",
  "abstract": "Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.",
  "collection-title": "UIST '18",
  "container-title": "Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
  "DOI": "10.1145/3242587.3242593",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5948-1",
  "page": "531–542",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Adasa: A Conversational In-Vehicle Digital Assistant for Advanced Driver Assistance Features",
  "title-short": "Adasa",
  "URL": "https://doi.org/10.1145/3242587.3242593",
  "author": [
    {
      "family": "Lin",
      "given": "Shih-Chieh"
    },
    {
      "family": "Hsu",
      "given": "Chang-Hong"
    },
    {
      "family": "Talamonti",
      "given": "Walter"
    },
    {
      "family": "Zhang",
      "given": "Yunqi"
    },
    {
      "family": "Oney",
      "given": "Steve"
    },
    {
      "family": "Mars",
      "given": "Jason"
    },
    {
      "family": "Tang",
      "given": "Lingjia"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development",
    "Testing",
    "Infrastructure"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c1"
  },
  "type": "paper-conference",
  "abstract": "Accurate estimations of the time of arrival of a flight prior to its landing are beneficial for most of the stakeholders in the air traffic management and control industry, as they could lead to reductions in potential safety risks and improvements in resources allocation. In this paper, and within the European Commission funded Transforming Transport project, we propose a methodology for real-time prediction of flight arrival times based on the application of machine learning techniques. For this purpose, we employ state-of-the-art data warehousing and broadcasting processes, that allow both the training of a regression machine learning model and the integration of its predictions of current flights on a real-time visualization tool set up for customer usage. The model only makes use of the information included in aircraft surveillance messages. Predictions obtained with such model are compared to those provided by other current services to observe the added value of the application of the proposed system on real-time operations.",
  "collection-title": "ECSA '18",
  "container-title": "Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings",
  "DOI": "10.1145/3241403.3241434",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6483-6",
  "page": "1–4",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Real-time prediction of flight arrival times using surveillance information",
  "URL": "https://doi.org/10.1145/3241403.3241434",
  "author": [
    {
      "family": "Muñoz",
      "given": "Andrés"
    },
    {
      "family": "Scarlatti",
      "given": "David"
    },
    {
      "family": "Costas",
      "given": "Pablo"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c2"
  },
  "type": "paper-conference",
  "abstract": "The evolution of telecom networks towards 5G comes with challenges that permeate the entire network structure and its management principles. In this paper, we outline how challenges such as the need to support different types of users and new network business models, the movement towards virtualization, as well as the need for more automation drive the requirement for a holistic and flexible way of managing the networks. We detail the architectural principles that underline such a cognitive management framework and we exemplify its use through a scenario based on software defined networking, where we combine machine learning, control and automation in the context of flexible resource provisioning in the Radio Access Network. Our experiments were conducted in collaboration with a major telecom operator and clearly show the advantages of introducing intelligence and automation into the network.",
  "container-title": "2017 8th International Conference on the Network of the Future (NOF)",
  "DOI": "10.1109/NOF.2017.8251220",
  "event": "2017 8th International Conference on the Network of the Future (NOF)",
  "page": "52-57",
  "source": "IEEE Xplore",
  "title": "Intelligent network management mechanisms as a step towards SG",
  "author": [
    {
      "family": "Bosneag",
      "given": "A."
    },
    {
      "family": "Wang",
      "given": "M. X."
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c3"
  },
  "type": "paper-conference",
  "abstract": "Network monitoring is necessary so as to ensure high reliability and availability in telecom networks. One of the main challenges posed by state-of-the-art monitoring tools is the creation of network baselines. Such baselines include thresholds that can be used to determine whether monitored values (with a given context, e.g. time) represent normal network operation or not. The size and complexity of current (and future) networks makes it infeasible to manually determine and set baselines for each network operator and metric, let alone adapting the thresholds to changes in network conditions. This leads to the use of default baselines and/or setting baselines only once and never changing them throughout the lifetime of network elements. This does not only cause inefficient operation, but could have implications for network reliability and availability. In this paper, we present the design, implementation, and evaluation of DARN: a collection of analytics and machine learning-based algorithms aimed at ensuring that network baselines are automatically adapted to different metric evolution. DARN has been comprehensively evaluated on a deployment with real traffic to confirm accuracy of generated baselines, a 22% improvement in accuracy due to baseline adaptation, and a 72% reduction in false alarms.",
  "container-title": "2018 4th IEEE Conference on Network Softwarization and Workshops (NetSoft)",
  "DOI": "10.1109/NETSOFT.2018.8460047",
  "event": "2018 4th IEEE Conference on Network Softwarization and Workshops (NetSoft)",
  "page": "37-45",
  "source": "IEEE Xplore",
  "title": "DARN: Dynamic Baselines for Real-time Network Monitoring",
  "title-short": "DARN",
  "author": [
    {
      "family": "Mijumbi",
      "given": "R."
    },
    {
      "family": "Asthana",
      "given": "A."
    },
    {
      "family": "Koivunen",
      "given": "M."
    },
    {
      "family": "Haiyong",
      "given": "F."
    },
    {
      "family": "Norman",
      "given": "Z."
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c4"
  },
  "type": "article-journal",
  "abstract": "Advanced medical imaging algorithms (such as bone removal, vessel segmentation, or a lung nodule detection) can provide extremely valuable information to the radiologists, but they might sometimes be very time consuming. Being able to run the algorithms in advance can be a possible solution. However, we do not know which algorithm to run on a given dataset before it is actually used. It is possible to manually insert matching rules for preprocessing algorithms, but it requires high maintenance and does not work well in practice. This paper presents a dynamic machine learning solution for predicting which advanced visualization (AV) algorithm needs to be applied on a given series. The system gets a handful of free text DICOM tags as an input and builds a model in the clinical setting. It incorporates a Bag of Words (BOW) feature extractor and a Random Forest classifier. The approach was tested on two datasets from clinical sites which use different languages and varying scanner models. We show that even without feature extraction, sensitivity of above 90% can be reached on both of them. By using BOW feature extractor, precision and sensitivity can usually be further improved. Even on a noisy and highly unbalanced dataset, only around 100 samples were needed to reach sensitivity of above 80% and specificity of above 97%. We show how the solution can be part of a Smart Preprocessing mechanism in a viewing software. Using such a system will ultimately minimize the time to launch studies and improve radiologists reading time efficiency.",
  "container-title": "Journal of Digital Imaging",
  "DOI": "10.1007/s10278-017-9999-9",
  "journalAbbreviation": "Journal of Digital Imaging",
  "source": "ResearchGate",
  "title": "Preprocessing Prediction of Advanced Algorithms for Medical Imaging",
  "volume": "31",
  "author": [
    {
      "family": "Fadia",
      "given": "Bella"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "AI Engineering",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c5"
  },
  "type": "article-journal",
  "abstract": "Operationalizing machine learning based security detections is extremely challenging, especially in a continuously evolving cloud environment. Conventional anomaly detection does not produce satisfactory results for analysts that are investigating security incidents in the cloud. Model evaluation alone presents its own set of problems due to a lack of benchmark datasets. When deploying these detections, we must deal with model compliance, localization, and data silo issues, among many others. We pose the problem of \"attack disruption\" as a way forward in the security data science space. In this paper, we describe the framework, challenges, and open questions surrounding the successful operationalization of machine learning based security detections in a cloud environment and provide some insights on how we have addressed them.",
  "container-title": "arXiv:1709.07095 [cs]",
  "note": "arXiv: 1709.07095",
  "source": "arXiv.org",
  "title": "Practical Machine Learning for Cloud Intrusion Detection: Challenges and the Way Forward",
  "title-short": "Practical Machine Learning for Cloud Intrusion Detection",
  "URL": "http://arxiv.org/abs/1709.07095",
  "author": [
    {
      "family": "Kumar",
      "given": "Ram Shankar Siva"
    },
    {
      "family": "Wicker",
      "given": "Andrew"
    },
    {
      "family": "Swann",
      "given": "Matt"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Infrastructure",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c6"
  },
  "type": "paper-conference",
  "abstract": "Crowdsourcing provides a scalable and efficient way to construct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowdworkers is often prohibitive even for seemingly simple concepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent labels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a missed opportunity to capture rich interpretations about data. We introduce Revolt, a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling. Revolt eliminates the burden of creating detailed label guidelines by harnessing crowd disagreements to identify ambiguous concepts and create rich structures (groups of semantically related items) for post-hoc label decisions. Experiments comparing Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label guidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt's ability to produce reusable structures that can accommodate a variety of label boundaries without requiring new data to be collected. Further comparisons of Revolt's collaborative and non-collaborative variants show that collaboration reaches higher label accuracy with lower monetary cost.",
  "DOI": "10.1145/3025453.3026044",
  "page": "2334-2346",
  "source": "ResearchGate",
  "title": "Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets",
  "title-short": "Revolt",
  "author": [
    {
      "family": "Chang",
      "given": "Joseph"
    },
    {
      "family": "Amershi",
      "given": "Saleema"
    },
    {
      "family": "Kamar",
      "given": "Ece"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c7"
  },
  "type": "article-journal",
  "abstract": "Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.",
  "container-title": "ACM SIGMOD Record",
  "DOI": "10.1145/3299887.3299891",
  "ISSN": "0163-5808",
  "issue": "2",
  "journalAbbreviation": "SIGMOD Rec.",
  "page": "17–28",
  "source": "June 2018",
  "title": "Data Lifecycle Challenges in Production Machine Learning: A Survey",
  "title-short": "Data Lifecycle Challenges in Production Machine Learning",
  "URL": "https://doi.org/10.1145/3299887.3299891",
  "volume": "47",
  "author": [
    {
      "family": "Polyzotis",
      "given": "Neoklis"
    },
    {
      "family": "Roy",
      "given": "Sudip"
    },
    {
      "family": "Whang",
      "given": "Steven Euijong"
    },
    {
      "family": "Zinkevich",
      "given": "Martin"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Model Development",
    "Model Deployment",
    "AI Engineering"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c8"
  },
  "type": "article-journal",
  "abstract": "Data cleaning and feature engineering are both common practices when developing machine learning (ML) models. However, developers are not always aware of best practices for preparing or transforming data for a given model type, which can lead to suboptimal representations of input features. To address this issue, we introduce the data linter, a new class of ML tool that automatically inspects ML data sets to 1) identify potential issues in the data and 2) suggest potentially useful feature transforms, for a given model type. As with traditional code linting, data linting automatically identifies potential issues or inefficiencies; codifies best practices and educates end-users about these practices through tool use; and can lead to quality improvements. In this paper, we provide a detailed description of data linting, describe our initial implementation of a data linter for deep neural networks, and report results suggesting the utility of using a data linter during ML model design.",
  "container-title": "undefined",
  "language": "en",
  "source": "www.semanticscholar.org",
  "title": "The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets",
  "title-short": "The Data Linter",
  "URL": "/paper/The-Data-Linter%3A-Lightweight-Automated-Sanity-for-Hynes-Sculley/80ba37dee9fdf340ce3b37eb1a09b6308e94169d",
  "author": [
    {
      "family": "Hynes",
      "given": "Nick"
    },
    {
      "family": "Sculley",
      "given": "D."
    },
    {
      "family": "Terry",
      "given": "Michael"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23c9"
  },
  "type": "paper-conference",
  "abstract": "The success of applications that process data critically depends on the quality of the ingested data. Completeness of a data source is essential in many cases. Yet, most missing value imputation approaches suffer from severe limitations. They are almost exclusively restricted to numerical data, and they either offer only simple imputation methods or are difficult to scale and maintain in production. Here we present a robust and scalable approach to imputation that extends to tables with non-numerical values, including unstructured text data in diverse languages. Experiments on public data sets as well as data sets sampled from a large product catalog in different languages (English and Japanese) demonstrate that the proposed approach is both scalable and yields more accurate imputations than previous approaches. Training on data sets with several million rows is a matter of minutes on a single machine. With a median imputation F1 score of 0.93 across a broad selection of data sets our approach achieves on average a 23-fold improvement compared to mode imputation. While our system allows users to apply state-of-the-art deep learning models if needed, we find that often simple linear n-gram models perform on par with deep learning methods at a much lower operational cost. The proposed method learns all parameters of the entire imputation pipeline automatically in an end-to-end fashion, rendering it attractive as a generic plugin both for engineers in charge of data pipelines where data completeness is relevant, as well as for practitioners without expertise in machine learning who need to impute missing values in tables with non-numerical data.",
  "collection-title": "CIKM '18",
  "container-title": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management",
  "DOI": "10.1145/3269206.3272005",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6014-2",
  "page": "2017–2025",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "\"Deep\" Learning for Missing Value Imputationin Tables with Non-Numerical Data",
  "URL": "https://doi.org/10.1145/3269206.3272005",
  "author": [
    {
      "family": "Biessmann",
      "given": "Felix"
    },
    {
      "family": "Salinas",
      "given": "David"
    },
    {
      "family": "Schelter",
      "given": "Sebastian"
    },
    {
      "family": "Schmidt",
      "given": "Philipp"
    },
    {
      "family": "Lange",
      "given": "Dustin"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Data Management",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ca"
  },
  "type": "paper-conference",
  "abstract": "As machine learning (ML) becomes increasingly popular, developers without deep experience in ML - who we will refer to as ML practitioners - are facing the need to diagnose problems with ML models. Yet successful diagnosis requires high-level expertise that practitioners lack. As in many complex data-oriented domains, visualization could help. This two-phase study explored the design of visualizations to aid ML diagnosis. In phase 1, twelve ML practitioners were asked to diagnose a model using ten state-of-the-art visualizations; seven design themes were identified. In phase 2, several design themes were embodied in an interactive visualization. The visualization was used to engage practitioners in a participatory design exercise that explored how they would carry out multi-step diagnosis using the visualization. Our findings provide design implications for tools that better support ML diagnosis by non-expert practitioners.",
  "container-title": "2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  "DOI": "10.1109/VLHCC.2016.7739669",
  "event": "2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  "note": "ISSN: 1943-6106",
  "page": "87-95",
  "source": "IEEE Xplore",
  "title": "Diagnostic visualization for non-expert machine learning practitioners: A design study",
  "title-short": "Diagnostic visualization for non-expert machine learning practitioners",
  "author": [
    {
      "family": "Chen",
      "given": "D."
    },
    {
      "family": "Bellamy",
      "given": "R. K. E."
    },
    {
      "family": "Malkin",
      "given": "P. K."
    },
    {
      "family": "Erickson",
      "given": "T."
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23cb"
  },
  "type": "paper-conference",
  "abstract": "Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference (prediction). A key challenge for platform providers is to guarantee response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource efficiency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of resource efficiency and SLA compliance. Our key contributions are (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning protocol for service instances. We evaluate Swayam on 15 popular services that were hosted on a production ML-as-a-service platform, for the following service-specific SLAs: for each service, at least 99% of requests must complete within the response-time threshold. Compared to a clairvoyant autoscaler that always satisfies the SLAs (i.e., even if there is a burst in the request rates), Swayam decreases resource utilization by up to 27%, while meeting the service-specific SLAs over 96% of the time during a three hour window. Microsoft Azure's Swayam-based framework was deployed in 2016 and has hosted over 100,000 services.",
  "DOI": "10.1145/3135974.3135993",
  "page": "109-120",
  "source": "ResearchGate",
  "title": "Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency",
  "title-short": "Swayam",
  "author": [
    {
      "family": "Gujarati",
      "given": "Arpan"
    },
    {
      "family": "Elnikety",
      "given": "Sameh"
    },
    {
      "family": "He",
      "given": "Yuxiong"
    },
    {
      "family": "McKinley",
      "given": "Kathryn"
    },
    {
      "family": "Brandenburg",
      "given": "Björn"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23cc"
  },
  "type": "paper-conference",
  "abstract": "There is huge demand for targeting complex and large-scale machine learning applications particularly those based on popular actively-maintained frameworks such as TensorFlow and CAFFE to a variety of platforms with accelerators ranging from high-end desktop GPUs to resource-constrained embedded or mobile GPUs, FPGAs, and DSPs. However, to deliver good performance different platforms may require different algorithms or data structures, yet code should be easily portable and reused as much as possible across different devices. The open SYCL standard addresses this by providing parallel processing through a single-source programming model enabling the same standard C++ code to be used on the CPU and accelerator. This allows high-level C++ abstractions and templates to be used to quickly configure device and host code to cover specific features of the platform. By targeting OpenCL, SYCL enables C++ applications such as TensorFlow to run efficiently on OpenCL devices without having to write OpenCL code.",
  "collection-title": "IWOCL '18",
  "container-title": "Proceedings of the International Workshop on OpenCL",
  "DOI": "10.1145/3204919.3204926",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-6439-3",
  "page": "1–4",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "TensorFlow Acceleration on ARM Hikey Board",
  "URL": "https://doi.org/10.1145/3204919.3204926",
  "author": [
    {
      "family": "Goli",
      "given": "Mehdi"
    },
    {
      "family": "Iwanski",
      "given": "Luke"
    },
    {
      "family": "Lawson",
      "given": "John"
    },
    {
      "family": "Dolinsky",
      "given": "Uwe"
    },
    {
      "family": "Richards",
      "given": "Andrew"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23cd"
  },
  "type": "article-journal",
  "abstract": "Debuggability is important in the development of machine-learning (ML) systems. Several widely-used ML libraries, such as TensorFlow and Theano, are based on dataflow graphs. While offering important benefits such as facilitating distributed training, the dataflow graph paradigm makes the debugging of model issues more challenging compared to debugging in the more conventional procedural paradigm. In this paper, we present the design of the TensorFlow Debugger (tfdbg), a specialized debugger for ML models written in TensorFlow. tfdbg provides features to inspect runtime dataflow graphs and the state of the intermediate graph elements (&quot;tensors&quot;), as well as simulating stepping on the graph. We will discuss the application of this debugger in development and testing use cases.",
  "container-title": "undefined",
  "language": "en",
  "source": "www.semanticscholar.org",
  "title": "TensorFlow Debugger: Debugging Dataflow Graphs for Machine Learning",
  "title-short": "TensorFlow Debugger",
  "URL": "/paper/TensorFlow-Debugger%3A-Debugging-Dataflow-Graphs-for-Cai-Breck/7a214ca0bc344f45f2a116a316ea01c9abc285e7",
  "author": [
    {
      "family": "Cai",
      "given": "Shanqing"
    },
    {
      "family": "Breck",
      "given": "Eric"
    },
    {
      "family": "Nielsen",
      "given": "Eric"
    },
    {
      "family": "Salib",
      "given": "M."
    },
    {
      "family": "Sculley",
      "given": "D."
    }
  ],
  "year": "2016",
  "DB": "Lwakatare",
  "categorie": [
    "Testing",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23ce"
  },
  "type": "article-journal",
  "abstract": "Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. The unique challenge of TaaS is that it must satisfy a wide range of customers who have no experience and resources to tune DL hyper-parameters, and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. IBM Watson Natural Language Classifier (NLC) service, the most popular IBM cognitive service used by thousands of enterprise-level clients around the globe, is a typical TaaS service. By evaluating the NLC workloads, we show that only the conservative hyper-parameter setup (e.g., small mini-batch size and small learning rate) can guarantee acceptable model accuracy for a wide range of customers. We further justify theoretically why such a setup guarantees better model convergence in general. Unfortunately, the small mini-batch size causes a high volume of communication traffic in a parameter-server based system. We characterize the high communication bandwidth requirement of TaaS using representative industrial deep learning workloads and demonstrate that none of the state-of-the-art scale-up or scale-out solutions can satisfy such a requirement. We then present GaDei, an optimized shared-memory based scale-up parameter server design. We prove that the designed protocol is deadlock-free and it processes each gradient exactly once. Our implementation is evaluated on both commercial benchmarks and public benchmarks to demonstrate that it significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy and our implementation reaches near the best possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance.",
  "container-title": "arXiv:1611.06213 [cs, stat]",
  "note": "arXiv: 1611.06213",
  "source": "arXiv.org",
  "title": "GaDei: On Scale-up Training As A Service For Deep Learning",
  "title-short": "GaDei",
  "URL": "http://arxiv.org/abs/1611.06213",
  "author": [
    {
      "family": "Zhang",
      "given": "Wei"
    },
    {
      "family": "Feng",
      "given": "Minwei"
    },
    {
      "family": "Zheng",
      "given": "Yunhui"
    },
    {
      "family": "Ren",
      "given": "Yufei"
    },
    {
      "family": "Wang",
      "given": "Yandong"
    },
    {
      "family": "Liu",
      "given": "Ji"
    },
    {
      "family": "Liu",
      "given": "Peng"
    },
    {
      "family": "Xiang",
      "given": "Bing"
    },
    {
      "family": "Zhang",
      "given": "Li"
    },
    {
      "family": "Zhou",
      "given": "Bowen"
    },
    {
      "family": "Wang",
      "given": "Fei"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23cf"
  },
  "type": "paper-conference",
  "abstract": "We argue for the necessity of managing the metadata and lineage of common artifacts in machine learning (ML). We discuss a recently presented lightweight system built for this task, which accelerates users in their ML workflows, and provides a basis for comparability and repeatability of ML experiments. This system tracks the lineage of produced artifacts in ML workloads and automatically extracts metadata such as hyperparameters of models, schemas of datasets and layouts of deep neural networks. It provides a general declarative representation of common ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.",
  "source": "ResearchGate",
  "title": "Declarative Metadata Management: A Missing Piece in End-To-End Machine Learning",
  "title-short": "Declarative Metadata Management",
  "author": [
    {
      "family": "Klein",
      "given": "Thoralf"
    },
    {
      "family": "Seufert",
      "given": "Stephan"
    },
    {
      "family": "Schelter",
      "given": "Sebastian"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d0"
  },
  "type": "paper-conference",
  "DOI": "10.1109/ICSTW.2018.00060",
  "page": "273-278",
  "source": "ResearchGate",
  "title": "A Test Architecture for Machine Learning Product",
  "author": [
    {
      "family": "Nishi",
      "given": "Yasuharu"
    },
    {
      "family": "Masuda",
      "given": "Satoshi"
    },
    {
      "family": "Ogawa",
      "given": "Hideto"
    },
    {
      "family": "Uetsuki",
      "given": "Keiji"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "AI Engineering",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d1"
  },
  "type": "article-journal",
  "abstract": "Deep learning, driven by large neural network models, is overtaking traditional machine learning methods for understanding unstructured and perceptual data domains such as speech, text, and vision. At the same time, the “As-a-Service”-based business model for the cloud is fundamentally transforming the information technology industry. These two trends, deep learning and “As-a-Service,” are colliding to give rise to a new business model for cognitive application delivery: deep learning as a service in the cloud. In this paper, we discuss the details of the software architecture behind IBM's deep learning as a service (DLaaS). DLaaS provides developers the flexibility to use popular deep learning libraries—such as Caffe, Torch, and TensorFlow—in the cloud in a scalable and resilient manner with minimal effort. The platform uses a distribution and orchestration layer that facilitates learning from a large amount of data in a reasonable amount of time across compute nodes. A resource provisioning layer enables flexible job management on heterogeneous resources, such as graphics processing units and central processing units, in an infrastructure-as-a-service cloud.",
  "container-title": "IBM Journal of Research and Development",
  "DOI": "10.1147/JRD.2017.2716578",
  "journalAbbreviation": "IBM Journal of Research and Development",
  "page": "10:1-10:11",
  "source": "ResearchGate",
  "title": "IBM Deep Learning Service",
  "volume": "61",
  "author": [
    {
      "family": "Bhattacharjee",
      "given": "Bishwaranjan"
    },
    {
      "family": "Boag",
      "given": "Scott"
    },
    {
      "family": "Doshi",
      "given": "Chandani"
    },
    {
      "family": "Dube",
      "given": "Parijat"
    },
    {
      "family": "Herta",
      "given": "Ben"
    },
    {
      "family": "Isahagian",
      "given": "Vatche"
    },
    {
      "family": "Jayaram",
      "given": "Kamesh"
    },
    {
      "family": "Khalaf",
      "given": "Rania"
    },
    {
      "family": "Krishna",
      "given": "Avesh"
    },
    {
      "family": "Li",
      "given": "Yu"
    },
    {
      "family": "Muthusamy",
      "given": "Vinod"
    },
    {
      "family": "Puri",
      "given": "Ruchir"
    },
    {
      "family": "Ren",
      "given": "Yufei"
    },
    {
      "family": "Rosenberg",
      "given": "Florian"
    },
    {
      "family": "Seelam",
      "given": "Seetharami"
    },
    {
      "family": "Wang",
      "given": "Yandong"
    },
    {
      "family": "Zhang",
      "given": "Jian"
    },
    {
      "family": "Zhang",
      "given": "Li"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Testing",
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d2"
  },
  "type": "article-journal",
  "abstract": "Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker's debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.",
  "container-title": "CHI",
  "DOI": "10.1145/2702123.2702509",
  "source": "Semantic Scholar",
  "title": "ModelTracker: Redesigning Performance Analysis Tools for Machine Learning",
  "title-short": "ModelTracker",
  "author": [
    {
      "family": "Amershi",
      "given": "Saleema"
    },
    {
      "family": "Chickering",
      "given": "D. M."
    },
    {
      "family": "Drucker",
      "given": "S."
    },
    {
      "family": "Lee",
      "given": "B."
    },
    {
      "family": "Simard",
      "given": "P."
    },
    {
      "family": "Suh",
      "given": "Jina"
    }
  ],
  "year": "2015",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d3"
  },
  "type": "paper-conference",
  "abstract": "Demand is mounting in the industry for scalable GPU-based deep learning systems. Unfortunately, existing training applications built atop popular deep learning frameworks, including Caffe, Theano, and Torch, etc, are incapable of conducting distributed GPU training over large-scale clusters. To remedy such a situation, this paper presents Nexus, a platform that allows existing deep learning frameworks to easily scale out to multiple machines without sacrificing model accuracy. Nexus leverages recently proposed distributed parameter management architecture to orchestrate distributed training by a large number of learners spread across the cluster. Through characterizing the run-time behavior of existing single-node based applications, Nexus is equipped with a suite of optimization schemes, including hierarchical and hybrid parameter aggregation, enhanced network and computation layer, and quality-guided communication adjustment, etc, to strengthen the communication channels and resource utilization. Empirical evaluations with a diverse set of deep learning applications demonstrate that Nexus is easy to integrate and can deliver efficient distributed training services to major deep learning frameworks. In addition, Nexus's optimization schemes are highly effective to shorten the training time with targeted accuracy bounds.",
  "container-title": "2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
  "DOI": "10.1109/MASCOTS.2017.34",
  "event": "2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",
  "note": "ISSN: 2375-0227",
  "page": "12-21",
  "source": "IEEE Xplore",
  "title": "Nexus: Bringing Efficient and Scalable Training to Deep Learning Frameworks",
  "title-short": "Nexus",
  "author": [
    {
      "family": "Wang",
      "given": "Y."
    },
    {
      "family": "Zhang",
      "given": "L."
    },
    {
      "family": "Ren",
      "given": "Y."
    },
    {
      "family": "Zhang",
      "given": "W."
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d4"
  },
  "type": "book",
  "abstract": "Runway is a cloud-native tool for managing machine learning experiments and their associated models. The iterative nature of developing models results in a large number of experiments and models that are often managed in an ad hoc manner. Runway is a workflow and framework independent tool that centrally manages and maintains metadata and links to artifacts needed to reproduce models and experiments. Runway provides a web dashboard with multiple levels of visualizations to evaluate performance and enable side-by-side comparisons of models and experiments.",
  "source": "ResearchGate",
  "title": "Runway: machine learning model experiment management tool",
  "title-short": "Runway",
  "author": [
    {
      "family": "Tsay",
      "given": "Jason"
    },
    {
      "family": "Mummert",
      "given": "Todd"
    },
    {
      "family": "Bobroff",
      "given": "Norman"
    },
    {
      "family": "Braz",
      "given": "Alan"
    },
    {
      "family": "Westerink",
      "given": "Peter"
    },
    {
      "family": "Hirzel",
      "given": "Martin"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d5"
  },
  "type": "article-journal",
  "abstract": "We present a framework for specifying, training, evaluating, and deploying machine learning models. Our focus is on simplifying cutting edge machine learning for practitioners in order to bring such technologies into production. Recognizing the fast evolution of the field of deep learning, we make no attempt to capture the design space of all possible model architectures in a domain- specific language (DSL) or similar configuration language. We allow users to write code to define their models, but provide abstractions that guide develop- ers to write models in ways conducive to productionization. We also provide a unifying Estimator interface, making it possible to write downstream infrastructure (e.g. distributed training, hyperparameter tuning) independent of the model implementation. We balance the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures. To make out of the box models flexible and usable across a wide range of problems, these canned Estimators are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data. We discuss our experience in using this framework in re- search and production environments, and show the impact on code health, maintainability, and development speed.",
  "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/3097983.3098171",
  "note": "arXiv: 1708.02637",
  "page": "1763-1771",
  "source": "arXiv.org",
  "title": "TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks",
  "title-short": "TensorFlow Estimators",
  "URL": "http://arxiv.org/abs/1708.02637",
  "author": [
    {
      "family": "Cheng",
      "given": "Heng-Tze"
    },
    {
      "family": "Haque",
      "given": "Zakaria"
    },
    {
      "family": "Hong",
      "given": "Lichan"
    },
    {
      "family": "Ispir",
      "given": "Mustafa"
    },
    {
      "family": "Mewald",
      "given": "Clemens"
    },
    {
      "family": "Polosukhin",
      "given": "Illia"
    },
    {
      "family": "Roumpos",
      "given": "Georgios"
    },
    {
      "family": "Sculley",
      "given": "D."
    },
    {
      "family": "Smith",
      "given": "Jamie"
    },
    {
      "family": "Soergel",
      "given": "David"
    },
    {
      "family": "Tang",
      "given": "Yuan"
    },
    {
      "family": "Tucker",
      "given": "Philipp"
    },
    {
      "family": "Wicke",
      "given": "Martin"
    },
    {
      "family": "Xia",
      "given": "Cassandra"
    },
    {
      "family": "Xie",
      "given": "Jianwei"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Testing",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d6"
  },
  "type": "article-journal",
  "abstract": "We describe TensorFlow-Serving, a system to serve machine learning models inside Google which is also available in the cloud and via open-source. It is extremely flexible in terms of the types of ML platforms it supports, and ways to integrate with systems that convey new models and updated versions from training to serving. At the same time, the core code paths around model lookup and inference have been carefully optimized to avoid performance pitfalls observed in naive implementations. Google uses it in many production deployments, including a multi-tenant model hosting service called TFS^2.",
  "container-title": "arXiv:1712.06139 [cs]",
  "note": "arXiv: 1712.06139",
  "source": "arXiv.org",
  "title": "TensorFlow-Serving: Flexible, High-Performance ML Serving",
  "title-short": "TensorFlow-Serving",
  "URL": "http://arxiv.org/abs/1712.06139",
  "author": [
    {
      "family": "Olston",
      "given": "Christopher"
    },
    {
      "family": "Fiedel",
      "given": "Noah"
    },
    {
      "family": "Gorovoy",
      "given": "Kiril"
    },
    {
      "family": "Harmsen",
      "given": "Jeremiah"
    },
    {
      "family": "Lao",
      "given": "Li"
    },
    {
      "family": "Li",
      "given": "Fangwei"
    },
    {
      "family": "Rajashekhar",
      "given": "Vinu"
    },
    {
      "family": "Ramesh",
      "given": "Sukriti"
    },
    {
      "family": "Soyke",
      "given": "Jordan"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d7"
  },
  "type": "paper-conference",
  "abstract": "Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.\nWe present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.\nWe present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.",
  "DOI": "10.1145/3097983.3098021",
  "page": "1387-1395",
  "source": "ResearchGate",
  "title": "TFX: A TensorFlow-Based Production-Scale Machine Learning Platform",
  "title-short": "TFX",
  "author": [
    {
      "family": "Baylor",
      "given": "Denis"
    },
    {
      "family": "Koc",
      "given": "Levent"
    },
    {
      "family": "Koo",
      "given": "Chiu"
    },
    {
      "family": "Lew",
      "given": "Lukasz"
    },
    {
      "family": "Mewald",
      "given": "Clemens"
    },
    {
      "family": "Modi",
      "given": "Akshay"
    },
    {
      "family": "Polyzotis",
      "given": "Neoklis"
    },
    {
      "family": "Ramesh",
      "given": "Sukriti"
    },
    {
      "family": "Roy",
      "given": "Sudip"
    },
    {
      "family": "Whang",
      "given": "Steven"
    },
    {
      "family": "Wicke",
      "given": "Martin"
    },
    {
      "family": "Breck",
      "given": "Eric"
    },
    {
      "family": "Wilkiewicz",
      "given": "Jarek"
    },
    {
      "family": "Zhang",
      "given": "Xin"
    },
    {
      "family": "Zinkevich",
      "given": "Martin"
    },
    {
      "family": "Cheng",
      "given": "Heng-Tze"
    },
    {
      "family": "Fiedel",
      "given": "Noah"
    },
    {
      "family": "Foo",
      "given": "Chuan"
    },
    {
      "family": "Haque",
      "given": "Zakaria"
    },
    {
      "family": "Jain",
      "given": "Vihan"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Model Deployment",
    "Infrastructure",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d8"
  },
  "type": "paper-conference",
  "DOI": "10.1109/BigData.2017.8258038",
  "page": "1123-1132",
  "source": "ResearchGate",
  "title": "The ML test score: A rubric for ML production readiness and technical debt reduction",
  "title-short": "The ML test score",
  "author": [
    {
      "family": "Breck",
      "given": "Eric"
    },
    {
      "family": "Cai",
      "given": "Shanqing"
    },
    {
      "family": "Nielsen",
      "given": "Eric"
    },
    {
      "family": "Salib",
      "given": "Michael"
    },
    {
      "family": "Sculley",
      "given": "D."
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Model Deployment",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23d9"
  },
  "type": "paper-conference",
  "abstract": "End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to find which datasets can be reused and which should be recomputed.\nIn this paper we build upon previous work to produce derivations of datasets to ensure that multiple versions of a pipeline can run in parallel while minimizing the amount of redundant computations. Our extensions include partial derivations to simplify navigation and reuse, explicit support for schema changes of pipelines, and a central registry of running pipelines to coordinate upgrading pipelines between teams.",
  "DOI": "10.1145/3076246.3076248",
  "page": "1-9",
  "source": "ResearchGate",
  "title": "Versioning for End-to-End Machine Learning Pipelines",
  "author": [
    {
      "family": "Weide",
      "given": "Tom"
    },
    {
      "family": "Papadopoulos",
      "given": "Dimitris"
    },
    {
      "family": "Smirnov",
      "given": "Oleg"
    },
    {
      "family": "Zielinski",
      "given": "Michal"
    },
    {
      "family": "Kasteren",
      "given": "Tim",
      "non-dropping-particle": "van"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23da"
  },
  "type": "article-journal",
  "abstract": "Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.",
  "container-title": "Proceedings of the VLDB Endowment",
  "DOI": "10.14778/3229863.3229867",
  "ISSN": "2150-8097",
  "issue": "12",
  "journalAbbreviation": "Proc. VLDB Endow.",
  "page": "1781–1794",
  "source": "August 2018",
  "title": "Automating large-scale data quality verification",
  "URL": "https://doi.org/10.14778/3229863.3229867",
  "volume": "11",
  "author": [
    {
      "family": "Schelter",
      "given": "Sebastian"
    },
    {
      "family": "Lange",
      "given": "Dustin"
    },
    {
      "family": "Schmidt",
      "given": "Philipp"
    },
    {
      "family": "Celikel",
      "given": "Meltem"
    },
    {
      "family": "Biessmann",
      "given": "Felix"
    },
    {
      "family": "Grafberger",
      "given": "Andreas"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Deployment"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23db"
  },
  "type": "paper-conference",
  "abstract": "This paper describes the challenges involved in arguing the safety of highly automated driving functions which make use of machine learning techniques. An assurance case structure is used to highlight the systems engineering and validation considerations when applying machine learning methods for highly automated driving. Particular focus is placed on addressing functional insufficiencies in the perception functions based on convolutional neural networks and possible types of evidence that can be used to mitigate against such risks.",
  "DOI": "10.1007/978-3-319-66284-8_1",
  "ISBN": "978-3-319-66283-1",
  "page": "5-16",
  "source": "ResearchGate",
  "title": "Making the Case for Safety of Machine Learning in Highly Automated Driving",
  "author": [
    {
      "family": "Burton",
      "given": "Simon"
    },
    {
      "family": "Gauerhof",
      "given": "Lydia"
    },
    {
      "family": "Heinzemann",
      "given": "Christian"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "AI Engineering",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23dc"
  },
  "type": "paper-conference",
  "DOI": "10.1109/ICCAD.2017.8203877",
  "page": "908-913",
  "source": "ResearchGate",
  "title": "Deep learning challenges and solutions with Xilinx FPGAs",
  "author": [
    {
      "family": "Delaye",
      "given": "Elliott"
    },
    {
      "family": "Sirasao",
      "given": "Ashish"
    },
    {
      "family": "Dudha",
      "given": "Chaithanya"
    },
    {
      "family": "Das",
      "given": "Sabya"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23dd"
  },
  "type": "paper-conference",
  "abstract": "Google Drive is a cloud storage and collaboration service used by hundreds of millions of users around the world. Quick Access is a new feature in Google Drive that surfaces the most relevant documents when a user visits the home screen. Our metrics show that users locate their documents in half the time with this feature compared to previous approaches. The development of Quick Access illustrates many general challenges and constraints associated with practical machine learning such as protecting user privacy, working with data services that are not designed with machine learning in mind, and evolving product definitions. We believe that the lessons learned from this experience will be useful to practitioners tackling a wide range of applied machine learning problems.",
  "collection-title": "KDD '17",
  "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
  "DOI": "10.1145/3097983.3098048",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4887-4",
  "page": "1643–1651",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Quick Access: Building a Smart Experience for Google Drive",
  "title-short": "Quick Access",
  "URL": "https://doi.org/10.1145/3097983.3098048",
  "author": [
    {
      "family": "Tata",
      "given": "Sandeep"
    },
    {
      "family": "Popescul",
      "given": "Alexandrin"
    },
    {
      "family": "Najork",
      "given": "Marc"
    },
    {
      "family": "Colagrosso",
      "given": "Mike"
    },
    {
      "family": "Gibbons",
      "given": "Julian"
    },
    {
      "family": "Green",
      "given": "Alan"
    },
    {
      "family": "Mah",
      "given": "Alexandre"
    },
    {
      "family": "Smith",
      "given": "Michael"
    },
    {
      "family": "Garg",
      "given": "Divanshu"
    },
    {
      "family": "Meyer",
      "given": "Cayden"
    },
    {
      "family": "Kan",
      "given": "Reuben"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Model Deployment",
    "Infrastructure",
    "Testing"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23de"
  },
  "type": "paper-conference",
  "abstract": "We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.",
  "collection-title": "CCS '17",
  "container-title": "Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security",
  "DOI": "10.1145/3133956.3133982",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-4946-8",
  "page": "1175–1191",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Practical Secure Aggregation for Privacy-Preserving Machine Learning",
  "URL": "https://doi.org/10.1145/3133956.3133982",
  "author": [
    {
      "family": "Bonawitz",
      "given": "Keith"
    },
    {
      "family": "Ivanov",
      "given": "Vladimir"
    },
    {
      "family": "Kreuter",
      "given": "Ben"
    },
    {
      "family": "Marcedone",
      "given": "Antonio"
    },
    {
      "family": "McMahan",
      "given": "H. Brendan"
    },
    {
      "family": "Patel",
      "given": "Sarvar"
    },
    {
      "family": "Ramage",
      "given": "Daniel"
    },
    {
      "family": "Segal",
      "given": "Aaron"
    },
    {
      "family": "Seth",
      "given": "Karn"
    }
  ],
  "year": "2017",
  "DB": "Lwakatare",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development",
    "Model Deployment"
  ]
},{
  "_id": {
    "$oid": "60db41a4b3da9102496b23df"
  },
  "type": "paper-conference",
  "abstract": "Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.",
  "collection-title": "EuroSys '18",
  "container-title": "Proceedings of the Thirteenth EuroSys Conference",
  "DOI": "10.1145/3190508.3190551",
  "event-place": "New York, NY, USA",
  "ISBN": "978-1-4503-5584-1",
  "page": "1–15",
  "publisher": "Association for Computing Machinery",
  "publisher-place": "New York, NY, USA",
  "source": "ACM Digital Library",
  "title": "Dynamic control flow in large-scale machine learning",
  "URL": "https://doi.org/10.1145/3190508.3190551",
  "author": [
    {
      "family": "Yu",
      "given": "Yuan"
    },
    {
      "family": "Abadi",
      "given": "Martín"
    },
    {
      "family": "Barham",
      "given": "Paul"
    },
    {
      "family": "Brevdo",
      "given": "Eugene"
    },
    {
      "family": "Burrows",
      "given": "Mike"
    },
    {
      "family": "Davis",
      "given": "Andy"
    },
    {
      "family": "Dean",
      "given": "Jeff"
    },
    {
      "family": "Ghemawat",
      "given": "Sanjay"
    },
    {
      "family": "Harley",
      "given": "Tim"
    },
    {
      "family": "Hawkins",
      "given": "Peter"
    },
    {
      "family": "Isard",
      "given": "Michael"
    },
    {
      "family": "Kudlur",
      "given": "Manjunath"
    },
    {
      "family": "Monga",
      "given": "Rajat"
    },
    {
      "family": "Murray",
      "given": "Derek"
    },
    {
      "family": "Zheng",
      "given": "Xiaoqiang"
    }
  ],
  "year": "2018",
  "DB": "Lwakatare",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e0"
  },
  "type": "article-journal",
  "abstract": "The application of Artificial Intelligence (AI) tools in different domains are becoming mandatory for all companies wishing to excel in their industries. One major challenge for a successful application of AI is to combine the machine learning (ML) expertise with the domain knowledge to have the best results applying AI tools. Domain specialists have an understanding of the data and how it can impact their decisions. ML experts have the ability to use AI-based tools dealing with large amounts of data and generating insights for domain experts. But without a deep understanding of the data, ML experts are not able to tune their models to get optimal results for a specific domain. Therefore, domain experts are key users for ML tools and the explainability of those AI tools become an essential feature in that context. There are a lot of efforts to research AI explainability for different contexts, users and goals. In this position paper, we discuss interesting findings about how ML experts can express concerns about AI explainability while defining features of an ML tool to be developed for a specific domain. We analyze data from two brainstorm sessions done to discuss the functionalities of an ML tool to support geoscientists (domain experts) on analyzing seismic data (domain-specific data) with ML resources.",
  "container-title": "arXiv:2002.12450 [cs]",
  "note": "arXiv: 2002.12450",
  "source": "arXiv.org",
  "title": "Do ML Experts Discuss Explainability for AI Systems? A discussion case in the industry for a domain-specific solution",
  "title-short": "Do ML Experts Discuss Explainability for AI Systems?",
  "URL": "http://arxiv.org/abs/2002.12450",
  "author": [
    {
      "family": "Ferreira",
      "given": "Juliana Jansen"
    },
    {
      "family": "Monteiro",
      "given": "Mateus de Souza"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality",
    "Project Management"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e1"
  },
  "type": "article-journal",
  "abstract": "The Bayesian treatment of neural networks dictates that a prior distribution is speciﬁed over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a large number of parameters, and the choice of these priors has an uncontrolled eﬀect on the induced functional prior, which is the distribution of the functions obtained by sampling the parameters from their prior distribution. We argue that this is a hugely limiting aspect of Bayesian deep learning, and this work tackles this limitation in a practical and eﬀective way. Our proposal is to reason in terms of functional priors, which are easier to elicit, and to “tune” the priors of neural network parameters in a way that they reﬂect such functional priors. Gaussian processes oﬀer a rigorous framework to deﬁne prior distributions over functions, and we propose a novel and robust framework to match their prior with the functional prior of neural networks based on the minimization of their Wasserstein distance. We provide vast experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling oﬀers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches. We consider this work a considerable step in the direction of making the long-standing challenge of carrying out a fully Bayesian treatment of neural networks, including convolutional neural networks, a concrete possibility.",
  "container-title": "arXiv:2011.12829 [cs, stat]",
  "language": "en",
  "note": "arXiv: 2011.12829",
  "source": "arXiv.org",
  "title": "All You Need is a Good Functional Prior for Bayesian Deep Learning",
  "URL": "http://arxiv.org/abs/2011.12829",
  "author": [
    {
      "family": "Tran",
      "given": "Ba-Hien"
    },
    {
      "family": "Rossi",
      "given": "Simone"
    },
    {
      "family": "Milios",
      "given": "Dimitrios"
    },
    {
      "family": "Filippone",
      "given": "Maurizio"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e2"
  },
  "type": "article-journal",
  "abstract": "Existing approaches for the design of interpretable agent behavior consider different measures of interpretability in isolation. In this paper we posit that, in the design and deployment of human-aware agents in the real world, notions of interpretability are just some among many considerations; and the techniques developed in isolation lack two key properties to be useful when considered together: they need to be able to 1) deal with their mutually competing properties; and 2) an open world where the human is not just there to interpret behavior in one specific form. To this end, we consider three well-known instances of interpretable behavior studied in existing literature -- namely, explicability, legibility, and predictability -- and propose a revised model where all these behaviors can be meaningfully modeled together. We will highlight interesting consequences of this unified model and motivate, through results of a user study, why this revision is necessary.",
  "container-title": "arXiv:2011.10920 [cs]",
  "note": "arXiv: 2011.10920",
  "source": "arXiv.org",
  "title": "A Bayesian Account of Measures of Interpretability in Human-AI Interaction",
  "URL": "http://arxiv.org/abs/2011.10920",
  "author": [
    {
      "family": "Sreedharan",
      "given": "Sarath"
    },
    {
      "family": "Kulkarni",
      "given": "Anagha"
    },
    {
      "family": "Chakraborti",
      "given": "Tathagata"
    },
    {
      "family": "Smith",
      "given": "David E."
    },
    {
      "family": "Kambhampati",
      "given": "Subbarao"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e3"
  },
  "type": "article-journal",
  "abstract": "A vital component of trust and transparency in intelligent systems built on machine learning and artificial intelligence is the development of clear, understandable documentation. However, such systems are notorious for their complexity and opaqueness making quality documentation a non-trivial task. Furthermore, little is known about what makes such documentation \"good.\" In this paper, we propose and evaluate a set of quality dimensions to identify in what ways this type of documentation falls short. Then, using those dimensions, we evaluate three different approaches for eliciting intelligent system documentation. We show how the dimensions identify shortcomings in such documentation and posit how such dimensions can be use to further enable users to provide documentation that is suitable to a given persona or use case.",
  "container-title": "arXiv:2011.08774 [cs]",
  "note": "arXiv: 2011.08774",
  "source": "arXiv.org",
  "title": "Towards evaluating and eliciting high-quality documentation for intelligent systems",
  "URL": "http://arxiv.org/abs/2011.08774",
  "author": [
    {
      "family": "Piorkowski",
      "given": "David"
    },
    {
      "family": "González",
      "given": "Daniel"
    },
    {
      "family": "Richards",
      "given": "John"
    },
    {
      "family": "Houde",
      "given": "Stephanie"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e4"
  },
  "type": "article-journal",
  "abstract": "The success of deep learning in recent years have led to a significant increase in interest and prevalence for its adoption to tackle financial services tasks. One particular question that often arises as a barrier to adopting deep learning for financial services is whether the developed financial deep learning models are fair in their predictions, particularly in light of strong governance and regulatory compliance requirements in the financial services industry. A fundamental aspect of fairness that has not been explored in financial deep learning is the concept of trust, whose variations may point to an egocentric view of fairness and thus provide insights into the fairness of models. In this study we explore the feasibility and utility of a multi-scale trust quantification strategy to gain insights into the fairness of a financial deep learning model, particularly under different scenarios at different scales. More specifically, we conduct multi-scale trust quantification on a deep neural network for the purpose of credit card default prediction to study: 1) the overall trustworthiness of the model 2) the trust level under all possible prediction-truth relationships, 3) the trust level across the spectrum of possible predictions, 4) the trust level across different demographic groups (e.g., age, gender, and education), and 5) distribution of overall trust for an individual prediction scenario. The insights for this proof-of-concept study demonstrate that such a multi-scale trust quantification strategy may be helpful for data scientists and regulators in financial services as part of the verification and certification of financial deep learning solutions to gain insights into fairness and trust of these solutions.",
  "container-title": "arXiv:2011.01961 [cs, q-fin]",
  "note": "arXiv: 2011.01961",
  "source": "arXiv.org",
  "title": "Insights into Fairness through Trust: Multi-scale Trust Quantification for Financial Deep Learning",
  "title-short": "Insights into Fairness through Trust",
  "URL": "http://arxiv.org/abs/2011.01961",
  "author": [
    {
      "family": "Wong",
      "given": "Alexander"
    },
    {
      "family": "Hryniowski",
      "given": "Andrew"
    },
    {
      "family": "Wang",
      "given": "Xiao Yu"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e5"
  },
  "type": "article-journal",
  "abstract": "The use of machine learning (ML) in health care raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of health care. Specifically, we frame ethics of ML in health care through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to post-deployment considerations. We close by summarizing recommendations to address these challenges.",
  "container-title": "arXiv:2009.10576 [cs]",
  "note": "arXiv: 2009.10576",
  "source": "arXiv.org",
  "title": "Ethical Machine Learning in Health Care",
  "URL": "http://arxiv.org/abs/2009.10576",
  "author": [
    {
      "family": "Chen",
      "given": "Irene Y."
    },
    {
      "family": "Pierson",
      "given": "Emma"
    },
    {
      "family": "Rose",
      "given": "Sherri"
    },
    {
      "family": "Joshi",
      "given": "Shalmali"
    },
    {
      "family": "Ferryman",
      "given": "Kadija"
    },
    {
      "family": "Ghassemi",
      "given": "Marzyeh"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e6"
  },
  "type": "article-journal",
  "abstract": "Despite the empirical success of using adversarial training to defend deep learning models against adversarial perturbations, so far, it still remains rather unclear what the principles are behind the existence of adversarial perturbations, and what adversarial training does to the neural network to remove them.",
  "container-title": "arXiv:2005.10190 [cs, math, stat]",
  "language": "en",
  "note": "arXiv: 2005.10190",
  "source": "arXiv.org",
  "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning",
  "title-short": "Feature Purification",
  "URL": "http://arxiv.org/abs/2005.10190",
  "author": [
    {
      "family": "Allen-Zhu",
      "given": "Zeyuan"
    },
    {
      "family": "Li",
      "given": "Yuanzhi"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e7"
  },
  "type": "article-journal",
  "abstract": "Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efﬁcient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment (DFA) to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our ﬁndings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to ﬁne-tuned backpropagation. When a larger gap between DFA and backpropagation exists, like in Transformers, we attribute this to a need to rethink common practices for large and complex architectures. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",
  "container-title": "arXiv:2006.12878 [cs, stat]",
  "language": "en",
  "note": "arXiv: 2006.12878",
  "source": "arXiv.org",
  "title": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures",
  "URL": "http://arxiv.org/abs/2006.12878",
  "author": [
    {
      "family": "Launay",
      "given": "Julien"
    },
    {
      "family": "Poli",
      "given": "Iacopo"
    },
    {
      "family": "Boniface",
      "given": "François"
    },
    {
      "family": "Krzakala",
      "given": "Florent"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e8"
  },
  "type": "article-journal",
  "abstract": "Many internet applications are powered by machine learned models, which are usually trained on labeled datasets obtained through either implicit / explicit user feedback signals or human judgments. Since societal biases may be present in the generation of such datasets, it is possible for the trained models to be biased, thereby resulting in potential discrimination and harms for disadvantaged groups. Motivated by the need for understanding and addressing algorithmic bias in web-scale ML systems and the limitations of existing fairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework for scalable computation of fairness metrics as part of large ML systems. We highlight the key requirements in deployed settings, and present the design of our fairness measurement system. We discuss the challenges encountered in incorporating fairness tools in practice and the lessons learned during deployment at LinkedIn. Finally, we provide open problems based on practical experience.",
  "container-title": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management",
  "DOI": "10.1145/3340531.3412705",
  "note": "arXiv: 2008.07433",
  "page": "2773-2780",
  "source": "arXiv.org",
  "title": "LiFT: A Scalable Framework for Measuring Fairness in ML Applications",
  "title-short": "LiFT",
  "URL": "http://arxiv.org/abs/2008.07433",
  "author": [
    {
      "family": "Vasudevan",
      "given": "Sriram"
    },
    {
      "family": "Kenthapadi",
      "given": "Krishnaram"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23e9"
  },
  "type": "article-journal",
  "abstract": "Prior studies have unveiled the vulnerability of the deep neural networks in the context of adversarial machine learning, leading to great recent attention into this area. One interesting question that has yet to be fully explored is the biasvariance relationship of adversarial machine learning, which can potentially provide deeper insights into this behaviour. The notion of bias and variance is one of the main approaches to analyze and evaluate the generalization and reliability of a machine learning model. Although it has been extensively used in other machine learning models, it is not well explored in the ﬁeld of deep learning and it is even less explored in the area of adversarial machine learning. In this study, we investigate the effect of adversarial machine learning on the bias and variance of a trained deep neural network and analyze how adversarial perturbations can affect the generalization of a network. We derive the bias-variance trade-off for both classiﬁcation and regression applications based on two main loss functions: (i) mean squared error (MSE), and (ii) cross-entropy. Furthermore, we perform quantitative analysis with both simulated and real data to empirically evaluate consistency with the derived bias-variance tradeoffs. Our analysis sheds light on why the deep neural networks have poor performance under adversarial perturbation from a bias-variance point of view and how this type of perturbation would change the performance of a network. Moreover, given these new theoretical ﬁndings, we introduce a new adversarial machine learning algorithm with lower computational complexity than well-known adversarial machine learning strategies (e.g., PGD) while providing a high success rate in fooling deep neural networks in lower perturbation magnitudes.",
  "container-title": "arXiv:2008.00138 [cs, stat]",
  "language": "en",
  "note": "arXiv: 2008.00138",
  "source": "arXiv.org",
  "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
  "title-short": "Vulnerability Under Adversarial Machine Learning",
  "URL": "http://arxiv.org/abs/2008.00138",
  "author": [
    {
      "family": "Aboutalebi",
      "given": "Hossein"
    },
    {
      "family": "Shafiee",
      "given": "Mohammad Javad"
    },
    {
      "family": "Karg",
      "given": "Michelle"
    },
    {
      "family": "Scharfenberger",
      "given": "Christian"
    },
    {
      "family": "Wong",
      "given": "Alexander"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23ea"
  },
  "type": "article-journal",
  "abstract": "Artificial intelligence (AI) has been successfully applied in numerous scientific domains. In biomedicine, AI has already shown tremendous potential, e.g. in the interpretation of next-generation sequencing data and in the design of clinical decision support systems. However, training an AI model on sensitive data raises concerns about the privacy of individual participants. For example, summary statistics of a genome-wide association study can be used to determine the presence or absence of an individual in a given dataset. This considerable privacy risk has led to restrictions in accessing genomic and other biomedical data, which is detrimental for collaborative research and impedes scientific progress. Hence, there has been a substantial effort to develop AI methods that can learn from sensitive data while protecting individuals' privacy. This paper provides a structured overview of recent advances in privacy-preserving AI techniques in biomedicine. It places the most important state-of-the-art approaches within a unified taxonomy and discusses their strengths, limitations, and open problems. As the most promising direction, we suggest combining federated machine learning as a more scalable approach with other additional privacy preserving techniques. This would allow to merge the advantages to provide privacy guarantees in a distributed way for biomedical applications. Nonetheless, more research is necessary as hybrid approaches pose new challenges such as additional network or computation overhead.",
  "container-title": "arXiv:2007.11621 [cs]",
  "note": "arXiv: 2007.11621",
  "source": "arXiv.org",
  "title": "Privacy-preserving Artificial Intelligence Techniques in Biomedicine",
  "URL": "http://arxiv.org/abs/2007.11621",
  "author": [
    {
      "family": "Torkzadehmahani",
      "given": "Reihaneh"
    },
    {
      "family": "Nasirigerdeh",
      "given": "Reza"
    },
    {
      "family": "Blumenthal",
      "given": "David B."
    },
    {
      "family": "Kacprowski",
      "given": "Tim"
    },
    {
      "family": "List",
      "given": "Markus"
    },
    {
      "family": "Matschinske",
      "given": "Julian"
    },
    {
      "family": "Späth",
      "given": "Julian"
    },
    {
      "family": "Wenke",
      "given": "Nina Kerstin"
    },
    {
      "family": "Bihari",
      "given": "Béla"
    },
    {
      "family": "Frisch",
      "given": "Tobias"
    },
    {
      "family": "Hartebrodt",
      "given": "Anne"
    },
    {
      "family": "Hausschild",
      "given": "Anne-Christin"
    },
    {
      "family": "Heider",
      "given": "Dominik"
    },
    {
      "family": "Holzinger",
      "given": "Andreas"
    },
    {
      "family": "Hötzendorfer",
      "given": "Walter"
    },
    {
      "family": "Kastelitz",
      "given": "Markus"
    },
    {
      "family": "Mayer",
      "given": "Rudolf"
    },
    {
      "family": "Nogales",
      "given": "Cristian"
    },
    {
      "family": "Pustozerova",
      "given": "Anastasia"
    },
    {
      "family": "Röttger",
      "given": "Richard"
    },
    {
      "family": "Schmidt",
      "given": "Harald H. H. W."
    },
    {
      "family": "Schwalber",
      "given": "Ameli"
    },
    {
      "family": "Tschohl",
      "given": "Christof"
    },
    {
      "family": "Wohner",
      "given": "Andrea"
    },
    {
      "family": "Baumbach",
      "given": "Jan"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23eb"
  },
  "type": "article-journal",
  "abstract": "In user targeting automation systems, concept drift in input data is one of the main challenges. It deteriorates model performance on new data over time. Previous research on concept drift mostly proposed model retraining after observing performance decreases. However, this approach is suboptimal because the system fixes the problem only after suffering from poor performance on new data. Here, we introduce an adversarial validation approach to concept drift problems in user targeting automation systems. With our approach, the system detects concept drift in new data before making inference, trains a model, and produces predictions adapted to the new data. We show that our approach addresses concept drift effectively with the AutoML3 Lifelong Machine Learning challenge data as well as in Uber’s internal user targeting automation system, MaLTA.",
  "container-title": "arXiv:2004.03045 [cs, stat]",
  "language": "en",
  "note": "arXiv: 2004.03045",
  "source": "arXiv.org",
  "title": "Adversarial Validation Approach to Concept Drift Problem in User Targeting Automation Systems at Uber",
  "URL": "http://arxiv.org/abs/2004.03045",
  "author": [
    {
      "family": "Pan",
      "given": "Jing"
    },
    {
      "family": "Pham",
      "given": "Vincent"
    },
    {
      "family": "Dorairaj",
      "given": "Mohan"
    },
    {
      "family": "Chen",
      "given": "Huigang"
    },
    {
      "family": "Lee",
      "given": "Jeong-Yoon"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Data Management",
    "AI Software Quality",
    "Model Development"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23ec"
  },
  "type": "article-journal",
  "abstract": "Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach, which has been proven on dynamic virtual machine provisioning, to request-based auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.",
  "container-title": "arXiv:2005.14410 [cs]",
  "note": "arXiv: 2005.14410",
  "source": "arXiv.org",
  "title": "AI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments",
  "title-short": "AI-based Resource Allocation",
  "URL": "http://arxiv.org/abs/2005.14410",
  "author": [
    {
      "family": "Schuler",
      "given": "Lucia"
    },
    {
      "family": "Jamil",
      "given": "Somaya"
    },
    {
      "family": "Kühl",
      "given": "Niklas"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "Infrastructure",
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23ed"
  },
  "type": "article-journal",
  "abstract": "We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists’ trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies – qualitative interviews, a controlled experiment, and a card-sorting task – to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.",
  "container-title": "Proceedings of the 25th International Conference on Intelligent User Interfaces",
  "DOI": "10.1145/3377325.3377501",
  "language": "en",
  "note": "arXiv: 2001.06509",
  "page": "297-307",
  "source": "arXiv.org",
  "title": "Trust in AutoML: Exploring Information Needs for Establishing Trust in Automated Machine Learning Systems",
  "title-short": "Trust in AutoML",
  "URL": "http://arxiv.org/abs/2001.06509",
  "author": [
    {
      "family": "Drozdal",
      "given": "Jaimie"
    },
    {
      "family": "Weisz",
      "given": "Justin"
    },
    {
      "family": "Wang",
      "given": "Dakuo"
    },
    {
      "family": "Dass",
      "given": "Gaurav"
    },
    {
      "family": "Yao",
      "given": "Bingsheng"
    },
    {
      "family": "Zhao",
      "given": "Changruo"
    },
    {
      "family": "Muller",
      "given": "Michael"
    },
    {
      "family": "Ju",
      "given": "Lin"
    },
    {
      "family": "Su",
      "given": "Hui"
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "AI Software Quality"
  ]
},{
  "_id": {
    "$oid": "60db41b6b3da9102496b23ee"
  },
  "type": "article-journal",
  "abstract": "Companies may be achieving only a third of the value they could be getting from data science in industry applications. In this paper, we propose a methodology for categorizing and answering ’The Big Three’ questions (what is going on, what is causing it, and what actions can I take that will optimize what I care about) using data science. The applications of data science seem to be nearly endless in today’s modern landscape, with each company jockeying for position in the new data and insights economy. Yet, data scientists seem to be solely focused on using classiﬁcation, regression, and clustering methods to answer the question ’what is going on’. Answering questions about why things are happening or how to take optimal actions to improve metrics are relegated to niche ﬁelds of research and generally neglected in industry data science analysis. We survey technical methods to answer these other important questions, describe areas in which some of these methods are being applied, and provide a practical example of how to apply our methodology and selected methods to a real business use case.",
  "container-title": "arXiv:2002.07069 [cs, stat]",
  "language": "en",
  "note": "arXiv: 2002.07069",
  "source": "arXiv.org",
  "title": "The Big Three: A Methodology to Increase Data Science ROI by Answering the Questions Companies Care About",
  "title-short": "The Big Three",
  "URL": "http://arxiv.org/abs/2002.07069",
  "author": [
    {
      "family": "Griffin",
      "given": "Daniel K."
    }
  ],
  "year": "2020",
  "DB": "AI Topics",
  "categorie": [
    "Model Development",
    "Data Management"
  ]
}]